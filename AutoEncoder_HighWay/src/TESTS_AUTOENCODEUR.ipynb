{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test de l'autoencodeur sur la base de données MNIST :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pdb\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import normalize\n",
    "from Models import *\n",
    "\n",
    "# Download mnist dataset\n",
    "from datamaestro import prepare_dataset\n",
    "ds = prepare_dataset(\"com.lecun.mnist\");\n",
    "train_images, train_labels = ds.train.images.data(), ds.train.labels.data()\n",
    "test_images, test_labels =  ds.test.images.data(), ds.test.labels.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path : where to save the state for checkpointing\n",
    "save_path = Path(\"autoencodeur.pch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writer at :  2020-11-08_20h45.28\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 32\n",
    "nb_epochs = 20\n",
    "eps = 0.001\n",
    "inputShape = 28*28 #784\n",
    "projectionShape = 80\n",
    "verbose = True\n",
    "\n",
    "# GPU/CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# train data loader\n",
    "train_data = DataLoader(\n",
    "    MonDataset(train_images, train_labels),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "# retreive the full test set (X not y)\n",
    "full_train_set = torch.tensor(train_images.reshape(train_images.shape[0],28*28)/255, dtype=torch.float32)\n",
    "# puch train set to device\n",
    "full_train_set = full_train_set.to(device)\n",
    "\n",
    "# test data loader (all the data since our hardware can handle it)\n",
    "test_data = DataLoader(\n",
    "    MonDataset(test_images, test_labels),\n",
    "    batch_size=test_images.__len__(),\n",
    "    shuffle=False,\n",
    ")\n",
    "# retreive the full train set (X not y)\n",
    "full_test_set = next(iter(test_data))[0]\n",
    "# puch test set to device\n",
    "full_test_set = full_test_set.to(device)\n",
    "\n",
    "\n",
    "#loss\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# writer tensorboard\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%M.%S\")\n",
    "print(\"writer at : \",current_time)\n",
    "writer = SummaryWriter(f'./autoencoder/LOSS_{current_time}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing\n",
    "if save_path.is_file():\n",
    "    # if we have a pending state\n",
    "    with  save_path.open(\"rb\") as fp:\n",
    "        state = torch.load(fp)\n",
    "        autoencodeur = state.model\n",
    "# otherwise, we start over\n",
    "else :\n",
    "    # model\n",
    "    autoencodeur = AutoEncoder(inputShape, projectionShape)\n",
    "    # push model to device\n",
    "    autoencodeur = autoencodeur.to(device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.Adam(autoencodeur.parameters(), eps)\n",
    "    \n",
    "    # save this new state\n",
    "    state = State(autoencodeur, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 6\n",
      "Itérations_train 11350: train_loss 0.07331560470163823\n",
      "Itérations_train 11350: test_loss 0.07247203342616558\n",
      "Itérations_train 11450: train_loss 0.07333903715014457\n",
      "Itérations_train 11450: test_loss 0.07244433768093586\n",
      "Itérations_train 11550: train_loss 0.0725051560252905\n",
      "Itérations_train 11550: test_loss 0.07240574613213539\n",
      "Itérations_train 11650: train_loss 0.07276112891733647\n",
      "Itérations_train 11650: test_loss 0.07238909430801868\n",
      "Itérations_train 11750: train_loss 0.07305547349154949\n",
      "Itérations_train 11750: test_loss 0.07237601451575756\n",
      "Itérations_train 11850: train_loss 0.07307086177170277\n",
      "Itérations_train 11850: test_loss 0.07232860796153545\n",
      "Itérations_train 11950: train_loss 0.07306029923260211\n",
      "Itérations_train 11950: test_loss 0.072303674146533\n",
      "Itérations_train 12050: train_loss 0.07304568126797677\n",
      "Itérations_train 12050: test_loss 0.07228427238762379\n",
      "Itérations_train 12150: train_loss 0.07247196577489376\n",
      "Itérations_train 12150: test_loss 0.07225789666175843\n",
      "Itérations_train 12250: train_loss 0.07287696942687034\n",
      "Itérations_train 12250: test_loss 0.07223598927259445\n",
      "Itérations_train 12350: train_loss 0.07274874769151211\n",
      "Itérations_train 12350: test_loss 0.07220610149204731\n",
      "Itérations_train 12450: train_loss 0.07305406019091606\n",
      "Itérations_train 12450: test_loss 0.07216091729700565\n",
      "Itérations_train 12550: train_loss 0.07257880888879299\n",
      "Itérations_train 12550: test_loss 0.07213731728494167\n",
      "Itérations_train 12650: train_loss 0.07340543948113919\n",
      "Itérations_train 12650: test_loss 0.07214066445827484\n",
      "Itérations_train 12750: train_loss 0.07273813053965568\n",
      "Itérations_train 12750: test_loss 0.07208362385630608\n",
      "Itérations_train 12850: train_loss 0.07221839517354965\n",
      "Itérations_train 12850: test_loss 0.072076231315732\n",
      "Itérations_train 12950: train_loss 0.07293617524206639\n",
      "Itérations_train 12950: test_loss 0.07206617906689644\n",
      "Itérations_train 13050: train_loss 0.0724202223867178\n",
      "Itérations_train 13050: test_loss 0.07202687069773674\n",
      "epoch = 7\n",
      "Itérations_train 13225: train_loss 0.07249946899712086\n",
      "Itérations_train 13225: test_loss 0.07201574936509132\n",
      "Itérations_train 13325: train_loss 0.07256852395832539\n",
      "Itérations_train 13325: test_loss 0.0720105627924204\n",
      "Itérations_train 13425: train_loss 0.07310216292738915\n",
      "Itérations_train 13425: test_loss 0.0719936753809452\n",
      "Itérations_train 13525: train_loss 0.07255045056343079\n",
      "Itérations_train 13525: test_loss 0.07196814730763436\n",
      "Itérations_train 13625: train_loss 0.07255162939429283\n",
      "Itérations_train 13625: test_loss 0.07191918581724167\n",
      "Itérations_train 13725: train_loss 0.07208732347935438\n",
      "Itérations_train 13725: test_loss 0.07191751554608344\n",
      "Itérations_train 13825: train_loss 0.07259990692138672\n",
      "Itérations_train 13825: test_loss 0.07188649646937847\n",
      "Itérations_train 13925: train_loss 0.07182515397667885\n",
      "Itérations_train 13925: test_loss 0.07187677450478076\n",
      "Itérations_train 14025: train_loss 0.07240751631557941\n",
      "Itérations_train 14025: test_loss 0.07187742456793784\n",
      "Itérations_train 14125: train_loss 0.07243765041232109\n",
      "Itérations_train 14125: test_loss 0.07187738202512264\n",
      "Itérations_train 14225: train_loss 0.07285303324460983\n",
      "Itérations_train 14225: test_loss 0.071830617710948\n",
      "Itérations_train 14325: train_loss 0.0720173966884613\n",
      "Itérations_train 14325: test_loss 0.07184227213263511\n",
      "Itérations_train 14425: train_loss 0.07251867454499006\n",
      "Itérations_train 14425: test_loss 0.07182147905230522\n",
      "Itérations_train 14525: train_loss 0.07268348678946496\n",
      "Itérations_train 14525: test_loss 0.07180103197693825\n",
      "Itérations_train 14625: train_loss 0.0726534091681242\n",
      "Itérations_train 14625: test_loss 0.07180378444492817\n",
      "Itérations_train 14725: train_loss 0.0720887117832899\n",
      "Itérations_train 14725: test_loss 0.0717599867284298\n",
      "Itérations_train 14825: train_loss 0.07256027191877365\n",
      "Itérations_train 14825: test_loss 0.0717564184218645\n",
      "Itérations_train 14925: train_loss 0.07252995684742927\n",
      "Itérations_train 14925: test_loss 0.07174095377326012\n",
      "epoch = 8\n",
      "Itérations_train 15100: train_loss 0.07243979811668395\n",
      "Itérations_train 15100: test_loss 0.07171008706092835\n",
      "Itérations_train 15200: train_loss 0.07181826002895832\n",
      "Itérations_train 15200: test_loss 0.0717064119875431\n",
      "Itérations_train 15300: train_loss 0.07215937435626983\n",
      "Itérations_train 15300: test_loss 0.07170481286942959\n",
      "Itérations_train 15400: train_loss 0.07222236156463623\n",
      "Itérations_train 15400: test_loss 0.07167983308434486\n",
      "Itérations_train 15500: train_loss 0.07258667565882206\n",
      "Itérations_train 15500: test_loss 0.07166466392576694\n",
      "Itérations_train 15600: train_loss 0.07229759730398655\n",
      "Itérations_train 15600: test_loss 0.07166653096675873\n",
      "Itérations_train 15700: train_loss 0.07249521762132645\n",
      "Itérations_train 15700: test_loss 0.07166681326925754\n",
      "Itérations_train 15800: train_loss 0.07194661483168602\n",
      "Itérations_train 15800: test_loss 0.0716270736604929\n",
      "Itérations_train 15900: train_loss 0.07234818413853646\n",
      "Itérations_train 15900: test_loss 0.07162894152104854\n",
      "Itérations_train 16000: train_loss 0.07187967218458652\n",
      "Itérations_train 16000: test_loss 0.07161778323352337\n",
      "Itérations_train 16100: train_loss 0.07240592397749424\n",
      "Itérations_train 16100: test_loss 0.07162558794021606\n",
      "Itérations_train 16200: train_loss 0.07242853216826915\n",
      "Itérations_train 16200: test_loss 0.07161100514233112\n",
      "Itérations_train 16300: train_loss 0.07225081436336041\n",
      "Itérations_train 16300: test_loss 0.0715704894065857\n",
      "Itérations_train 16400: train_loss 0.07242601118981838\n",
      "Itérations_train 16400: test_loss 0.07156599961221217\n",
      "Itérations_train 16500: train_loss 0.07190904896706343\n",
      "Itérations_train 16500: test_loss 0.07156883217394353\n",
      "Itérations_train 16600: train_loss 0.07172328270971776\n",
      "Itérations_train 16600: test_loss 0.07155529692769051\n",
      "Itérations_train 16700: train_loss 0.07192280501127243\n",
      "Itérations_train 16700: test_loss 0.07156250827014446\n",
      "Itérations_train 16800: train_loss 0.07232243470847606\n",
      "Itérations_train 16800: test_loss 0.07156001627445222\n",
      "epoch = 9\n",
      "Itérations_train 16975: train_loss 0.07206945963203908\n",
      "Itérations_train 16975: test_loss 0.07151481598615646\n",
      "Itérations_train 17075: train_loss 0.07243769258260727\n",
      "Itérations_train 17075: test_loss 0.0715107586979866\n",
      "Itérations_train 17175: train_loss 0.07200008571147919\n",
      "Itérations_train 17175: test_loss 0.07151080317795276\n",
      "Itérations_train 17275: train_loss 0.07227168716490269\n",
      "Itérations_train 17275: test_loss 0.0714923981577158\n",
      "Itérations_train 17375: train_loss 0.0724781670421362\n",
      "Itérations_train 17375: test_loss 0.07150507733225822\n",
      "Itérations_train 17475: train_loss 0.07154946364462375\n",
      "Itérations_train 17475: test_loss 0.07148032575845718\n",
      "Itérations_train 17575: train_loss 0.07174703910946846\n",
      "Itérations_train 17575: test_loss 0.07146615728735924\n",
      "Itérations_train 17675: train_loss 0.07241587050259113\n",
      "Itérations_train 17675: test_loss 0.07148997627198696\n",
      "Itérations_train 17775: train_loss 0.0717802207916975\n",
      "Itérations_train 17775: test_loss 0.07143399447202682\n",
      "Itérations_train 17875: train_loss 0.07193526074290275\n",
      "Itérations_train 17875: test_loss 0.07143259562551975\n",
      "Itérations_train 17975: train_loss 0.07205180749297142\n",
      "Itérations_train 17975: test_loss 0.07143194444477557\n",
      "Itérations_train 18075: train_loss 0.07213263459503651\n",
      "Itérations_train 18075: test_loss 0.0714269084483385\n",
      "Itérations_train 18175: train_loss 0.07188960637897253\n",
      "Itérations_train 18175: test_loss 0.07141379162669181\n",
      "Itérations_train 18275: train_loss 0.07193086922168732\n",
      "Itérations_train 18275: test_loss 0.07139983125030995\n",
      "Itérations_train 18375: train_loss 0.0718788693100214\n",
      "Itérations_train 18375: test_loss 0.0714051827788353\n",
      "Itérations_train 18475: train_loss 0.07202174954116344\n",
      "Itérations_train 18475: test_loss 0.07139456987380982\n",
      "Itérations_train 18575: train_loss 0.07268724083900452\n",
      "Itérations_train 18575: test_loss 0.07138666316866875\n",
      "Itérations_train 18675: train_loss 0.07244140863418579\n",
      "Itérations_train 18675: test_loss 0.07141040332615375\n",
      "epoch = 10\n",
      "Itérations_train 18850: train_loss 0.0718491593748331\n",
      "Itérations_train 18850: test_loss 0.07136394128203392\n",
      "Itérations_train 18950: train_loss 0.07205469638109208\n",
      "Itérations_train 18950: test_loss 0.07136706851422786\n",
      "Itérations_train 19050: train_loss 0.07230109870433807\n",
      "Itérations_train 19050: test_loss 0.07138398587703705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 19150: train_loss 0.0721411458402872\n",
      "Itérations_train 19150: test_loss 0.07138675332069397\n",
      "Itérations_train 19250: train_loss 0.07171055272221566\n",
      "Itérations_train 19250: test_loss 0.07136527232825757\n",
      "Itérations_train 19350: train_loss 0.07217791885137557\n",
      "Itérations_train 19350: test_loss 0.07133989118039608\n",
      "Itérations_train 19450: train_loss 0.0720610000193119\n",
      "Itérations_train 19450: test_loss 0.07134017460048199\n",
      "Itérations_train 19550: train_loss 0.071456803008914\n",
      "Itérations_train 19550: test_loss 0.0713442937284708\n",
      "Itérations_train 19650: train_loss 0.0720025659352541\n",
      "Itérations_train 19650: test_loss 0.07132169485092163\n",
      "Itérations_train 19750: train_loss 0.07149826839566231\n",
      "Itérations_train 19750: test_loss 0.07133240163326264\n",
      "Itérations_train 19850: train_loss 0.07177230287343264\n",
      "Itérations_train 19850: test_loss 0.07132580019533634\n",
      "Itérations_train 19950: train_loss 0.07218231230974198\n",
      "Itérations_train 19950: test_loss 0.07133088909089565\n",
      "Itérations_train 20050: train_loss 0.07190804030746221\n",
      "Itérations_train 20050: test_loss 0.07131429918110371\n",
      "Itérations_train 20150: train_loss 0.07179919045418501\n",
      "Itérations_train 20150: test_loss 0.07130684919655322\n",
      "Itérations_train 20250: train_loss 0.07193698085844517\n",
      "Itérations_train 20250: test_loss 0.0713004770874977\n",
      "Itérations_train 20350: train_loss 0.07212641648948193\n",
      "Itérations_train 20350: test_loss 0.07131935209035874\n",
      "Itérations_train 20450: train_loss 0.07183165829628706\n",
      "Itérations_train 20450: test_loss 0.07128234416246414\n",
      "Itérations_train 20550: train_loss 0.0719615164771676\n",
      "Itérations_train 20550: test_loss 0.07127858884632587\n",
      "epoch = 11\n",
      "Itérations_train 20725: train_loss 0.07232029832899571\n",
      "Itérations_train 20725: test_loss 0.0713089931756258\n",
      "Itérations_train 20825: train_loss 0.07192164089530706\n",
      "Itérations_train 20825: test_loss 0.07128037050366402\n",
      "Itérations_train 20925: train_loss 0.07182134080678225\n",
      "Itérations_train 20925: test_loss 0.07125200673937798\n",
      "Itérations_train 21025: train_loss 0.07187062576413154\n",
      "Itérations_train 21025: test_loss 0.07127471111714839\n",
      "Itérations_train 21125: train_loss 0.07126852288842202\n",
      "Itérations_train 21125: test_loss 0.07126481875777245\n",
      "Itérations_train 21225: train_loss 0.0717945196479559\n",
      "Itérations_train 21225: test_loss 0.0712745364755392\n",
      "Itérations_train 21325: train_loss 0.07157163973897696\n",
      "Itérations_train 21325: test_loss 0.07124236047267914\n",
      "Itérations_train 21425: train_loss 0.07181490506976843\n",
      "Itérations_train 21425: test_loss 0.07124114289879799\n",
      "Itérations_train 21525: train_loss 0.07146949604153634\n",
      "Itérations_train 21525: test_loss 0.07124747075140477\n",
      "Itérations_train 21625: train_loss 0.07161629036068916\n",
      "Itérations_train 21625: test_loss 0.07123765394091607\n",
      "Itérations_train 21725: train_loss 0.07132414057850837\n",
      "Itérations_train 21725: test_loss 0.07122862949967385\n",
      "Itérations_train 21825: train_loss 0.07213276855647564\n",
      "Itérations_train 21825: test_loss 0.0712264409661293\n",
      "Itérations_train 21925: train_loss 0.07238236889243126\n",
      "Itérations_train 21925: test_loss 0.07123595759272576\n",
      "Itérations_train 22025: train_loss 0.0720599963515997\n",
      "Itérations_train 22025: test_loss 0.07122085347771645\n",
      "Itérations_train 22125: train_loss 0.07202558666467666\n",
      "Itérations_train 22125: test_loss 0.07122815035283565\n",
      "Itérations_train 22225: train_loss 0.07198275923728943\n",
      "Itérations_train 22225: test_loss 0.07122851178050041\n",
      "Itérations_train 22325: train_loss 0.07178511127829551\n",
      "Itérations_train 22325: test_loss 0.07120161823928356\n",
      "Itérations_train 22425: train_loss 0.07175800591707229\n",
      "Itérations_train 22425: test_loss 0.0711989639699459\n",
      "epoch = 12\n",
      "Itérations_train 22600: train_loss 0.07186301603913307\n",
      "Itérations_train 22600: test_loss 0.07122476294636726\n",
      "Itérations_train 22700: train_loss 0.07161760590970516\n",
      "Itérations_train 22700: test_loss 0.07120697177946568\n",
      "Itérations_train 22800: train_loss 0.07153078399598599\n",
      "Itérations_train 22800: test_loss 0.07118891797959805\n",
      "Itérations_train 22900: train_loss 0.07166217137128114\n",
      "Itérations_train 22900: test_loss 0.07119555495679379\n",
      "Itérations_train 23000: train_loss 0.07203432224690914\n",
      "Itérations_train 23000: test_loss 0.07122374877333641\n",
      "Itérations_train 23100: train_loss 0.07195410285145044\n",
      "Itérations_train 23100: test_loss 0.07117753230035305\n",
      "Itérations_train 23200: train_loss 0.07193509086966515\n",
      "Itérations_train 23200: test_loss 0.07118918895721435\n",
      "Itérations_train 23300: train_loss 0.0721164108067751\n",
      "Itérations_train 23300: test_loss 0.07117045395076275\n",
      "Itérations_train 23400: train_loss 0.07191466756165027\n",
      "Itérations_train 23400: test_loss 0.07116640523076058\n",
      "Itérations_train 23500: train_loss 0.07167980469763278\n",
      "Itérations_train 23500: test_loss 0.07118446476757527\n",
      "Itérations_train 23600: train_loss 0.07235847845673561\n",
      "Itérations_train 23600: test_loss 0.07118705280125141\n",
      "Itérations_train 23700: train_loss 0.07194066025316716\n",
      "Itérations_train 23700: test_loss 0.07115682989358901\n",
      "Itérations_train 23800: train_loss 0.0712785167247057\n",
      "Itérations_train 23800: test_loss 0.07114711418747902\n",
      "Itérations_train 23900: train_loss 0.07146449305117131\n",
      "Itérations_train 23900: test_loss 0.07115377642214299\n",
      "Itérations_train 24000: train_loss 0.07193654052913188\n",
      "Itérations_train 24000: test_loss 0.0711600659787655\n",
      "Itérations_train 24100: train_loss 0.07226760447025299\n",
      "Itérations_train 24100: test_loss 0.07113378673791886\n",
      "Itérations_train 24200: train_loss 0.07152360133826732\n",
      "Itérations_train 24200: test_loss 0.07113983765244485\n",
      "Itérations_train 24300: train_loss 0.07089325219392777\n",
      "Itérations_train 24300: test_loss 0.07113049782812596\n",
      "epoch = 13\n",
      "Itérations_train 24475: train_loss 0.07122598730027675\n",
      "Itérations_train 24475: test_loss 0.07112125463783742\n",
      "Itérations_train 24575: train_loss 0.07138910971581935\n",
      "Itérations_train 24575: test_loss 0.07111251540482044\n",
      "Itérations_train 24675: train_loss 0.071937585324049\n",
      "Itérations_train 24675: test_loss 0.07113029174506665\n",
      "Itérations_train 24775: train_loss 0.07149168953299523\n",
      "Itérations_train 24775: test_loss 0.07114915110170841\n",
      "Itérations_train 24875: train_loss 0.07149806778877973\n",
      "Itérations_train 24875: test_loss 0.07111160218715668\n",
      "Itérations_train 24975: train_loss 0.0718363082408905\n",
      "Itérations_train 24975: test_loss 0.07113585017621517\n",
      "Itérations_train 25075: train_loss 0.07153807401657104\n",
      "Itérations_train 25075: test_loss 0.07113739863038063\n",
      "Itérations_train 25175: train_loss 0.07152588494122028\n",
      "Itérations_train 25175: test_loss 0.07112137049436569\n",
      "Itérations_train 25275: train_loss 0.07212237901985645\n",
      "Itérations_train 25275: test_loss 0.07114050783216953\n",
      "Itérations_train 25375: train_loss 0.07141102701425553\n",
      "Itérations_train 25375: test_loss 0.07111871227622032\n",
      "Itérations_train 25475: train_loss 0.07197511862963438\n",
      "Itérations_train 25475: test_loss 0.07112566396594047\n",
      "Itérations_train 25575: train_loss 0.07193498581647872\n",
      "Itérations_train 25575: test_loss 0.07111312456429005\n",
      "Itérations_train 25675: train_loss 0.07153994657099247\n",
      "Itérations_train 25675: test_loss 0.07108361572027207\n",
      "Itérations_train 25775: train_loss 0.07156109765172004\n",
      "Itérations_train 25775: test_loss 0.07110023714601993\n",
      "Itérations_train 25875: train_loss 0.07207584615796804\n",
      "Itérations_train 25875: test_loss 0.07110356494784355\n",
      "Itérations_train 25975: train_loss 0.0714027239382267\n",
      "Itérations_train 25975: test_loss 0.07109173029661178\n",
      "Itérations_train 26075: train_loss 0.07167138516902924\n",
      "Itérations_train 26075: test_loss 0.07106542602181434\n",
      "Itérations_train 26175: train_loss 0.07208771206438541\n",
      "Itérations_train 26175: test_loss 0.07107474036514759\n",
      "epoch = 14\n",
      "Itérations_train 26350: train_loss 0.0719004151970148\n",
      "Itérations_train 26350: test_loss 0.0710783015191555\n",
      "Itérations_train 26450: train_loss 0.07167045656591654\n",
      "Itérations_train 26450: test_loss 0.0711041134595871\n",
      "Itérations_train 26550: train_loss 0.07202777914702892\n",
      "Itérations_train 26550: test_loss 0.07111330918967723\n",
      "Itérations_train 26650: train_loss 0.07161528490483761\n",
      "Itérations_train 26650: test_loss 0.07108084887266158\n",
      "Itérations_train 26750: train_loss 0.07198505088686943\n",
      "Itérations_train 26750: test_loss 0.07107417300343513\n",
      "Itérations_train 26850: train_loss 0.07159121468663215\n",
      "Itérations_train 26850: test_loss 0.07105237536132336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 26950: train_loss 0.07144228644669055\n",
      "Itérations_train 26950: test_loss 0.07106476925313472\n",
      "Itérations_train 27050: train_loss 0.07130870193243027\n",
      "Itérations_train 27050: test_loss 0.07107259996235371\n",
      "Itérations_train 27150: train_loss 0.07207399688661098\n",
      "Itérations_train 27150: test_loss 0.07106060869991779\n",
      "Itérations_train 27250: train_loss 0.07170933932065963\n",
      "Itérations_train 27250: test_loss 0.07106531076133252\n",
      "Itérations_train 27350: train_loss 0.07166164964437485\n",
      "Itérations_train 27350: test_loss 0.07106759257614613\n",
      "Itérations_train 27450: train_loss 0.07115104213356972\n",
      "Itérations_train 27450: test_loss 0.07107386901974679\n",
      "Itérations_train 27550: train_loss 0.07153983041644096\n",
      "Itérations_train 27550: test_loss 0.07107738681137561\n",
      "Itérations_train 27650: train_loss 0.07097855307161809\n",
      "Itérations_train 27650: test_loss 0.07107189610600471\n",
      "Itérations_train 27750: train_loss 0.07126331131905317\n",
      "Itérations_train 27750: test_loss 0.07105377152562141\n",
      "Itérations_train 27850: train_loss 0.07204559992998838\n",
      "Itérations_train 27850: test_loss 0.07102514557540417\n",
      "Itérations_train 27950: train_loss 0.07194129124283791\n",
      "Itérations_train 27950: test_loss 0.07101884983479977\n",
      "Itérations_train 28050: train_loss 0.07145922876894474\n",
      "Itérations_train 28050: test_loss 0.07102329537272453\n",
      "epoch = 15\n",
      "Itérations_train 28225: train_loss 0.07198475401848554\n",
      "Itérations_train 28225: test_loss 0.07103505522012711\n",
      "Itérations_train 28325: train_loss 0.07187903679907322\n",
      "Itérations_train 28325: test_loss 0.07105915077030658\n",
      "Itérations_train 28425: train_loss 0.07134021110832692\n",
      "Itérations_train 28425: test_loss 0.07105621188879013\n",
      "Itérations_train 28525: train_loss 0.07125925071537495\n",
      "Itérations_train 28525: test_loss 0.07101241119205952\n",
      "Itérations_train 28625: train_loss 0.07138391993939877\n",
      "Itérations_train 28625: test_loss 0.07102027595043182\n",
      "Itérations_train 28725: train_loss 0.0713359959051013\n",
      "Itérations_train 28725: test_loss 0.07104412563145161\n",
      "Itérations_train 28825: train_loss 0.07169031336903572\n",
      "Itérations_train 28825: test_loss 0.0710272241383791\n",
      "Itérations_train 28925: train_loss 0.07121734216809272\n",
      "Itérations_train 28925: test_loss 0.07102559946477413\n",
      "Itérations_train 29025: train_loss 0.07160079948604108\n",
      "Itérations_train 29025: test_loss 0.07101178750395774\n",
      "Itérations_train 29125: train_loss 0.0717970633506775\n",
      "Itérations_train 29125: test_loss 0.07103288717567921\n",
      "Itérations_train 29225: train_loss 0.07222384609282016\n",
      "Itérations_train 29225: test_loss 0.07103148251771926\n",
      "Itérations_train 29325: train_loss 0.07170185785740614\n",
      "Itérations_train 29325: test_loss 0.0710495787858963\n",
      "Itérations_train 29425: train_loss 0.07147305123507977\n",
      "Itérations_train 29425: test_loss 0.07103506654500961\n",
      "Itérations_train 29525: train_loss 0.07127374336123467\n",
      "Itérations_train 29525: test_loss 0.07099746592342854\n",
      "Itérations_train 29625: train_loss 0.07172729305922985\n",
      "Itérations_train 29625: test_loss 0.07101360104978084\n",
      "Itérations_train 29725: train_loss 0.07225916683673858\n",
      "Itérations_train 29725: test_loss 0.0710032557696104\n",
      "Itérations_train 29825: train_loss 0.07127228818833828\n",
      "Itérations_train 29825: test_loss 0.07100422836840153\n",
      "Itérations_train 29925: train_loss 0.07126346930861473\n",
      "Itérations_train 29925: test_loss 0.07100538969039917\n",
      "epoch = 16\n",
      "Itérations_train 30100: train_loss 0.07162824667990207\n",
      "Itérations_train 30100: test_loss 0.07099988341331481\n",
      "Itérations_train 30200: train_loss 0.07145109258592129\n",
      "Itérations_train 30200: test_loss 0.07099903784692288\n",
      "Itérations_train 30300: train_loss 0.0720579045265913\n",
      "Itérations_train 30300: test_loss 0.07100585930049419\n",
      "Itérations_train 30400: train_loss 0.07161287724971771\n",
      "Itérations_train 30400: test_loss 0.07102101996541023\n",
      "Itérations_train 30500: train_loss 0.07201337851583958\n",
      "Itérations_train 30500: test_loss 0.07100881434977055\n",
      "Itérations_train 30600: train_loss 0.07182983562350273\n",
      "Itérations_train 30600: test_loss 0.07100938625633717\n",
      "Itérations_train 30700: train_loss 0.07183344647288323\n",
      "Itérations_train 30700: test_loss 0.07099987044930459\n",
      "Itérations_train 30800: train_loss 0.07164269961416721\n",
      "Itérations_train 30800: test_loss 0.07098735891282558\n",
      "Itérations_train 30900: train_loss 0.07116730481386185\n",
      "Itérations_train 30900: test_loss 0.07099378772079945\n",
      "Itérations_train 31000: train_loss 0.07120318189263344\n",
      "Itérations_train 31000: test_loss 0.07101226530969143\n",
      "Itérations_train 31100: train_loss 0.07155931644141673\n",
      "Itérations_train 31100: test_loss 0.07095941483974456\n",
      "Itérations_train 31200: train_loss 0.07187186121940613\n",
      "Itérations_train 31200: test_loss 0.07096954219043256\n",
      "Itérations_train 31300: train_loss 0.07115088187158108\n",
      "Itérations_train 31300: test_loss 0.07096643581986427\n",
      "Itérations_train 31400: train_loss 0.0713001187890768\n",
      "Itérations_train 31400: test_loss 0.07098277755081654\n",
      "Itérations_train 31500: train_loss 0.07132710888981819\n",
      "Itérations_train 31500: test_loss 0.07097186729311943\n",
      "Itérations_train 31600: train_loss 0.07131743364036083\n",
      "Itérations_train 31600: test_loss 0.07096262469887733\n",
      "Itérations_train 31700: train_loss 0.07160006538033485\n",
      "Itérations_train 31700: test_loss 0.07098032660782337\n",
      "Itérations_train 31800: train_loss 0.07136247165501118\n",
      "Itérations_train 31800: test_loss 0.07097617521882058\n",
      "epoch = 17\n",
      "Itérations_train 31975: train_loss 0.07083819277584552\n",
      "Itérations_train 31975: test_loss 0.07096338093280792\n",
      "Itérations_train 32075: train_loss 0.07150820001959801\n",
      "Itérations_train 32075: test_loss 0.0709463232755661\n",
      "Itérations_train 32175: train_loss 0.07230218097567559\n",
      "Itérations_train 32175: test_loss 0.07098286129534244\n",
      "Itérations_train 32275: train_loss 0.07189672105014325\n",
      "Itérations_train 32275: test_loss 0.07098121337592601\n",
      "Itérations_train 32375: train_loss 0.07152828507125378\n",
      "Itérations_train 32375: test_loss 0.07096821464598178\n",
      "Itérations_train 32475: train_loss 0.07133860804140568\n",
      "Itérations_train 32475: test_loss 0.07093703024089336\n",
      "Itérations_train 32575: train_loss 0.07159733206033707\n",
      "Itérations_train 32575: test_loss 0.07098622560501099\n",
      "Itérations_train 32675: train_loss 0.07147581554949284\n",
      "Itérations_train 32675: test_loss 0.07096263028681278\n",
      "Itérations_train 32775: train_loss 0.0716599553078413\n",
      "Itérations_train 32775: test_loss 0.07097694396972656\n",
      "Itérations_train 32875: train_loss 0.0713258621096611\n",
      "Itérations_train 32875: test_loss 0.07095970571041108\n",
      "Itérations_train 32975: train_loss 0.0717262752354145\n",
      "Itérations_train 32975: test_loss 0.07093097783625125\n",
      "Itérations_train 33075: train_loss 0.07117838591337204\n",
      "Itérations_train 33075: test_loss 0.07096654742956161\n",
      "Itérations_train 33175: train_loss 0.0713117266446352\n",
      "Itérations_train 33175: test_loss 0.07095872454345226\n",
      "Itérations_train 33275: train_loss 0.07149807833135129\n",
      "Itérations_train 33275: test_loss 0.07094295725226402\n",
      "Itérations_train 33375: train_loss 0.07116019822657109\n",
      "Itérations_train 33375: test_loss 0.07094605922698975\n",
      "Itérations_train 33475: train_loss 0.07151606343686581\n",
      "Itérations_train 33475: test_loss 0.07092548198997975\n",
      "Itérations_train 33575: train_loss 0.07195873565971851\n",
      "Itérations_train 33575: test_loss 0.07093837089836598\n",
      "Itérations_train 33675: train_loss 0.07154362201690674\n",
      "Itérations_train 33675: test_loss 0.07095755189657212\n",
      "epoch = 18\n",
      "Itérations_train 33850: train_loss 0.07139736346900463\n",
      "Itérations_train 33850: test_loss 0.0709473317861557\n",
      "Itérations_train 33950: train_loss 0.0716673617810011\n",
      "Itérations_train 33950: test_loss 0.07092995047569275\n",
      "Itérations_train 34050: train_loss 0.07117548152804375\n",
      "Itérations_train 34050: test_loss 0.07093339420855045\n",
      "Itérations_train 34150: train_loss 0.07069343917071819\n",
      "Itérations_train 34150: test_loss 0.07093699887394905\n",
      "Itérations_train 34250: train_loss 0.0712823823466897\n",
      "Itérations_train 34250: test_loss 0.07094035021960736\n",
      "Itérations_train 34350: train_loss 0.0718696303665638\n",
      "Itérations_train 34350: test_loss 0.07093299753963947\n",
      "Itérations_train 34450: train_loss 0.07183892518281937\n",
      "Itérations_train 34450: test_loss 0.07094495154917241\n",
      "Itérations_train 34550: train_loss 0.07091780319809914\n",
      "Itérations_train 34550: test_loss 0.0709502087533474\n",
      "Itérations_train 34650: train_loss 0.07181360714137554\n",
      "Itérations_train 34650: test_loss 0.07093933209776879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 34750: train_loss 0.07156762406229973\n",
      "Itérations_train 34750: test_loss 0.07094007670879364\n",
      "Itérations_train 34850: train_loss 0.07173777785152197\n",
      "Itérations_train 34850: test_loss 0.07092596657574177\n",
      "Itérations_train 34950: train_loss 0.0717395617812872\n",
      "Itérations_train 34950: test_loss 0.0709273286908865\n",
      "Itérations_train 35050: train_loss 0.0718519377708435\n",
      "Itérations_train 35050: test_loss 0.07091687761247158\n",
      "Itérations_train 35150: train_loss 0.07135405369102955\n",
      "Itérations_train 35150: test_loss 0.0709135527908802\n",
      "Itérations_train 35250: train_loss 0.07144000366330147\n",
      "Itérations_train 35250: test_loss 0.07090861432254314\n",
      "Itérations_train 35350: train_loss 0.07156197309494018\n",
      "Itérations_train 35350: test_loss 0.07091790474951268\n",
      "Itérations_train 35450: train_loss 0.07149364620447159\n",
      "Itérations_train 35450: test_loss 0.07093279659748078\n",
      "Itérations_train 35550: train_loss 0.07182177916169166\n",
      "Itérations_train 35550: test_loss 0.07090675994753838\n",
      "epoch = 19\n",
      "Itérations_train 35725: train_loss 0.07145263075828552\n",
      "Itérations_train 35725: test_loss 0.07090157829225063\n",
      "Itérations_train 35825: train_loss 0.0710133583843708\n",
      "Itérations_train 35825: test_loss 0.07091544024646282\n",
      "Itérations_train 35925: train_loss 0.07160837352275848\n",
      "Itérations_train 35925: test_loss 0.07091172270476818\n",
      "Itérations_train 36025: train_loss 0.07158026687800884\n",
      "Itérations_train 36025: test_loss 0.07092673167586326\n",
      "Itérations_train 36125: train_loss 0.07133209683001042\n",
      "Itérations_train 36125: test_loss 0.07091036699712276\n",
      "Itérations_train 36225: train_loss 0.07184953071177005\n",
      "Itérations_train 36225: test_loss 0.07091692492365836\n",
      "Itérations_train 36325: train_loss 0.0710238666832447\n",
      "Itérations_train 36325: test_loss 0.0708952534943819\n",
      "Itérations_train 36425: train_loss 0.07126971140503884\n",
      "Itérations_train 36425: test_loss 0.07090805873274802\n",
      "Itérations_train 36525: train_loss 0.0712603160738945\n",
      "Itérations_train 36525: test_loss 0.07092399716377258\n",
      "Itérations_train 36625: train_loss 0.07147635623812676\n",
      "Itérations_train 36625: test_loss 0.0708939230442047\n",
      "Itérations_train 36725: train_loss 0.07170260723680258\n",
      "Itérations_train 36725: test_loss 0.07088567577302456\n",
      "Itérations_train 36825: train_loss 0.07180294048041105\n",
      "Itérations_train 36825: test_loss 0.07088854730129242\n",
      "Itérations_train 36925: train_loss 0.07132497578859329\n",
      "Itérations_train 36925: test_loss 0.07090459033846855\n",
      "Itérations_train 37025: train_loss 0.07158551938831806\n",
      "Itérations_train 37025: test_loss 0.07088766716420651\n",
      "Itérations_train 37125: train_loss 0.07129391729831695\n",
      "Itérations_train 37125: test_loss 0.0708949252218008\n",
      "Itérations_train 37225: train_loss 0.07115858994424343\n",
      "Itérations_train 37225: test_loss 0.07089806914329529\n",
      "Itérations_train 37325: train_loss 0.07156731858849526\n",
      "Itérations_train 37325: test_loss 0.07089135207235814\n",
      "Itérations_train 37425: train_loss 0.0715468792617321\n",
      "Itérations_train 37425: test_loss 0.07090928919613361\n"
     ]
    }
   ],
   "source": [
    "# logging the train loss\n",
    "Nbatch_train_loss = 0\n",
    "epoch_train_loss = 0 # computed on the whole test set\n",
    "# logging the test loss (computed on the whole test set)\n",
    "Nbatch_test_loss = 0\n",
    "epoch_test_loss = 0\n",
    "loss_val_test = 0\n",
    "\n",
    "# visualize image decoding without any training\n",
    "# Images are of the format Channel (3) x Hauteur x Largeur\n",
    "sample_test = full_test_set[0:3]\n",
    "sample_test = sample_test.to(device)\n",
    "sample_test = autoencodeur(sample_test)\n",
    "images = sample_test.reshape((sample_test.shape[0],28,28)).unsqueeze(1).repeat(1,3,1,1)\n",
    "# Permet de fabriquer une grille d'images\n",
    "images = make_grid(images)\n",
    "# Affichage avec tensorboard\n",
    "writer.add_image('samples_by_epoch', images, 0)\n",
    "\n",
    "# start from where we stopped (self.epoch / 0 if no pending state)\n",
    "for epoch in range(state.epoch, nb_epochs):\n",
    "    # store in these variables sum of losses of N batches\n",
    "    Nbatch_test_loss = 0\n",
    "    Nbatch_train_loss = 0\n",
    "    if verbose : print(\"epoch = {}\".format(epoch))\n",
    "    for i, (image_batch, _) in enumerate(train_data) :\n",
    "        # total number of batches : checkpointing\n",
    "        state.iteration += 1\n",
    "        # grad to zero\n",
    "        state.optim.zero_grad()\n",
    "        # push batch to GPU/CPU\n",
    "        image_batch = image_batch.to(device)\n",
    "        # compute nn output : done on device\n",
    "        output = autoencodeur(image_batch)\n",
    "        # compute loss\n",
    "        loss_val = loss(output, image_batch)\n",
    "        # backpropagations\n",
    "        loss_val.backward()\n",
    "        # update the parameters\n",
    "        state.optim.step()\n",
    "        # train loss\n",
    "        Nbatch_train_loss += float(loss_val)\n",
    "\n",
    "        # test loss\n",
    "        # compute loss on test\n",
    "        with torch.no_grad():\n",
    "            loss_val_test = loss(autoencodeur(full_test_set),full_test_set)\n",
    "            Nbatch_test_loss += float(loss_val_test)      \n",
    "        # To visualize images\n",
    "        if (state.iteration-1)%100 == 0:\n",
    "            # Images are of the format Channel (3) x Hauteur x Largeur\n",
    "            sample_test = full_test_set[0:3]\n",
    "            sample_test = sample_test.to(device)\n",
    "            sample_test = autoencodeur(sample_test)\n",
    "            images = sample_test.reshape((sample_test.shape[0],28,28)).unsqueeze(1).repeat(1,3,1,1)\n",
    "            # Permet de fabriquer une grille d'images\n",
    "            images = make_grid(images)\n",
    "            # Affichage avec tensorboard\n",
    "            writer.add_image('samples_100_iterations', images, state.iteration)\n",
    "\n",
    "        \n",
    "        # log (after each 100 batches)\n",
    "        if ((i+1) % 100  == 0):\n",
    "            # verbose\n",
    "            if verbose :\n",
    "                print(f\"Itérations_train {state.iteration}: train_loss {Nbatch_train_loss/100}\")\n",
    "                print(f\"Itérations_train {state.iteration}: test_loss {Nbatch_test_loss/100}\")\n",
    "                \n",
    "            # log mean of losses after each 100 batches\n",
    "            writer.add_scalars('Loss/Nbatch_loss', {'mean_100_batches_train': Nbatch_train_loss/100,\n",
    "                                                     'mean_100_batches_test': Nbatch_test_loss/100,\n",
    "                                                    }, state.iteration)\n",
    "            # for the next 100 batches\n",
    "            Nbatch_test_loss = 0\n",
    "            Nbatch_train_loss = 0\n",
    "        \n",
    "    # log the loss for the epoch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # for the test get the last loss\n",
    "        epoch_test_loss = loss_val_test\n",
    "\n",
    "        # for the train compute loss on full train set\n",
    "        epoch_train_loss = loss(autoencodeur(full_train_set), full_train_set)\n",
    "\n",
    "        # log by epoch losses\n",
    "        writer.add_scalars('Loss/by_epoch_loss', {'by_epoch_loss_train': epoch_train_loss,\n",
    "                                                 'by_epoch_loss_test': epoch_test_loss,\n",
    "                                                }, state.epoch)  \n",
    "    \n",
    "    # visualize image decoding quality for each epoch\n",
    "    # Images are of the format Channel (3) x Hauteur x Largeur\n",
    "    sample_test = full_test_set[0:3]\n",
    "    sample_test = sample_test.to(device)\n",
    "    sample_test = autoencodeur(sample_test)\n",
    "    images = sample_test.reshape((sample_test.shape[0],28,28)).unsqueeze(1).repeat(1,3,1,1)\n",
    "    # Permet de fabriquer une grille d'images\n",
    "    images = make_grid(images)\n",
    "    # Affichage avec tensorboard\n",
    "    writer.add_image('samples_by_epoch', images, state.epoch+1)\n",
    "    \n",
    "    # save the state : checkpointing\n",
    "    with save_path.open(\"wb\") as fp :\n",
    "        state.epoch = state.epoch + 1\n",
    "        torch.save(state,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device is :  cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"device is : \",image_batch.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 6956), started 4:36:46 ago. (Use '!kill 6956' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-795d91f05c7b1565\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-795d91f05c7b1565\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open TensorBoard\n",
    "%tensorboard --logdir autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline \n",
    "example = full_test_set[110]\n",
    "data = example.cpu().reshape(28,28).detach().numpy()\n",
    "plt.imshow(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQCUlEQVR4nO3df5BV9XnH8c+zywKKYNigFBV/1FBTmlRottCosRJbq7Qz/phqZaYOpmnWpmrNVBsdmw7a6UytMVozTW1JJFLHoslEK5maVrLDjEnrIAuigKig4ghBiEPKT92fT//Yo7Pinucs99xf8H2/Znb27nnuuffh7n44957vOedr7i4AR7+WRjcAoD4IO5AIwg4kgrADiSDsQCLG1PPJxto4H68J9XxKICnv6YB6vcdGqpUKu5ldLOl+Sa2SvuPud0X3H68JmmsXlnlKAIFV3pVbq/htvJm1SvqWpEskzZS0wMxmVvp4AGqrzGf2OZK2uPvr7t4r6VFJl1anLQDVVibsJ0t6a9jP27JlH2JmnWbWbWbdfeop8XQAyqj53nh3X+zuHe7e0aZxtX46ADnKhH27pOnDfj4lWwagCZUJ+2pJM8zsDDMbK+lqScur0xaAaqt46M3d+83sBkn/raGhtyXuvrFqnQGoqlLj7O7+lKSnqtQLgBricFkgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEEHYgEXWdshlHHhsXz+Kz54rZYX3eV/83t3Zt+7PhumvfOyWsf+2HfxTWz7p/W26tf1vBfCbucf0IxJYdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEmNdxPHGStftcu7BuzwfJxsSHUtgnPxHW997TG9af+LWHw/o4y9+etKk1XLes7t6xubW7fv/KcN2BTZur3U5drPIu7fXdNlKt1EE1ZrZV0j5JA5L63b2jzOMBqJ1qHEE3z93fqcLjAKghPrMDiSgbdpf0tJmtMbPOke5gZp1m1m1m3X3qKfl0ACpV9m38ee6+3cxOlLTCzF5292eG38HdF0taLA3toCv5fAAqVGrL7u7bs++7JD0haU41mgJQfRWH3cwmmNnE929LukjShmo1BqC6yryNnyrpCTN7/3H+3d3/qypd4bBE55y/9ZefCdf90ZfvDuvHt8Rj4c/3TAzrf/7Qn+XWpj4Xj+FPv+PVsP7gqSvD+tlj382tvXvq8eG6YzeF5SNSxWF399clnV3FXgDUEENvQCIIO5AIwg4kgrADiSDsQCK4lHQVWFv+qZSj4X3xEFSRlukn5da+23l/uG57S/wn8LWd54X1Vy+cENan/1/+paRlI56J+YG1sz8b1nVjPPTWqvzHb+0ZjB/7KMSWHUgEYQcSQdiBRBB2IBGEHUgEYQcSQdiBRDDOXgc+MFDuAQpOM910ywm5tZlt8XMv2nlOWH/5nPi5vWdPWI+0/uqMsP5PX/qXsD6oeKz8u3vOyq2Nezmesrm/4BiAI3FKZ7bsQCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kgnH2KvD+voI7lBuTbZ10XFj/+3nfz63tG+wP1133V7PD+pjetWG96Jz0novzJ/Zd9q/3hetOaT0mrK9899iw/qMrfjO3NrDrjXDdwt/ZETgOz5YdSARhBxJB2IFEEHYgEYQdSARhBxJB2IFEMM5eDTUeU7XJHwvrBwbzp2xe3zs5XPedX89fV5JO7I0n6n39uni8+aXP/3NubZzFxw/0eXwu/o3L/jSsn/Ha6vziYMlrDByBCrfsZrbEzHaZ2YZhy9rNbIWZbc6+x39RABpuNG/jH5J08SHLbpPU5e4zJHVlPwNoYoVhd/dnJO0+ZPGlkpZmt5dKuqy6bQGotko/s0919x3Z7bclTc27o5l1SuqUpPGKj2UGUDul98a7u0vK3UPl7ovdvcPdO9oU7wwCUDuVhn2nmU2TpOz7ruq1BKAWKg37ckkLs9sLJT1ZnXYA1Ip5wRixmS2TdIGkKZJ2Slok6T8kfU/SqZLelHSVux+6E+8jJlm7z7ULy3WcoNYpHw/rA4/ln/f98K88Fq77XsHvf3zBeduTW8aH9TaLrzsf6Xo3Xvfe838vrPf/bEd+sQnPN6+GVd6lvb57xF9a4Q46d1+QUyK1wBGEw2WBRBB2IBGEHUgEYQcSQdiBRHCKazXU+LLCg3v2hfXtT+dPTXzsWfHw1Ymt8dBZkR6PL6Pd4/nTKq/qaQvXvXHJdWH9tF+sC+uyaFsWT/d8NA7NsWUHEkHYgUQQdiARhB1IBGEHEkHYgUQQdiARjLOPVjSWHo7nSmXHdL2vN6yf9JMDubV918dTNh9X0PpAME4uSVv64sf/4xe+kFubdkv87zr1jefC+uBAweWgo99L0e+s4DLWRyK27EAiCDuQCMIOJIKwA4kg7EAiCDuQCMIOJIJx9mqo9fS/BefL/+KT+dNqtRWsWzSOXqS9Jf63t/xn/gS/A6/F4+jlX9dy/7ajDVt2IBGEHUgEYQcSQdiBRBB2IBGEHUgEYQcSwTj7aJW5jnjBWHfLsfnj5JK075JPhfWbb300t1Y00rzfe8L6YMG/e2JL/Cf0B9c/k1t77pGPxc99IP88/VGJei+61v9RqHDLbmZLzGyXmW0YtuwOM9tuZuuyr/m1bRNAWaN5G/+QpItHWH6fu8/Kvp6qblsAqq0w7O7+jKTddegFQA2V2UF3g5m9mL3Nzz0A2sw6zazbzLr7FH8+BFA7lYb9AUlnSpolaYekb+Td0d0Xu3uHu3e0aVyFTwegrIrC7u473X3A3QclfVvSnOq2BaDaKgq7mU0b9uPlkjbk3RdAcygcZzezZZIukDTFzLZJWiTpAjObJcklbZUUT6SduNaJE8P6K3fODOvP/mHupyRJ0rjgGuiLdv52uO6Lt54d1t+bEs+h/hd3PhbWb2zPP2f93CVfDtc980tbw/rAvnje+qNxjvUyCsPu7gtGWPxgDXoBUEMcLgskgrADiSDsQCIIO5AIwg4kIp1TXMue0hgM49iY+GXccU18iuoLV94b1o+xY8L6377z6dzaK58bG67bdnBNXA+r0tcnXR3WL1/0zdza/5zzQLjulZ+9KayPfTruPZTgsBxbdiARhB1IBGEHEkHYgUQQdiARhB1IBGEHEpHOOHvZcdVgnH5wbjyO/vhX7w7rbRZfweee3WeF9dW/c1JubfDgz8N1yzph9d6wvmewt+LHtqJfWYJj5WWwZQcSQdiBRBB2IBGEHUgEYQcSQdiBRBB2IBHpjLOXFE2rPH/xynDdU8fEUzL/3TvxOP2zn4nX9/4ajqW3tIblzQvjy2S3Kf/4hO/v/0S47vjn3wzrA2EVh2LLDiSCsAOJIOxAIgg7kAjCDiSCsAOJIOxAIhhnf1/BdeX3X5Q/Fv6F47vCdQcLrr7+4zs/F9aP7V8V1iM2Lj5XXp+eEZa3z5sU1ldeEZ+r32L54/T/uPHz4bqnHXg9rOPwFG7ZzWy6ma00s5fMbKOZ3ZQtbzezFWa2Ofs+ufbtAqjUaN7G90u62d1nSvotSdeb2UxJt0nqcvcZkrqynwE0qcKwu/sOd1+b3d4naZOkkyVdKmlpdrelki6rUY8AquCwPrOb2emSZktaJWmqu+/ISm9LmpqzTqekTkkar/gYbwC1M+q98WZ2nKQfSPqKu3/oKoPu7pJGvPqfuy929w5372hTwc4iADUzqrCbWZuGgv6Iuz+eLd5pZtOy+jRJu2rTIoBqKHwbb2Ym6UFJm9x9+NzCyyUtlHRX9v3JmnQ4XHS65WDJEx4LLkvc2jOYWzvo8XO3BcNPkrR/4Z6wPunZXwrrB2ZNz61de1/8a5k7Pj49t8fj3vsKrub8N2+fn1s745b4MtT9Bw/GD47DMprP7OdKukbSejNbly27XUMh/56ZfVHSm5KuqkmHAKqiMOzu/lMp9woEF1a3HQC1wuGyQCIIO5AIwg4kgrADiSDsQCLM6zjt7SRr97l2ZO7AH3PKybm1s3/4VrjunSc+H9YHlT+GLxVPe3ycxafQRrb0x8/9rV3zwvpPls8O66d/c0NubWBvPM6Ow7fKu7TXd484esaWHUgEYQcSQdiBRBB2IBGEHUgEYQcSQdiBRHAp6VHq/9nbubU1150drvsP34nHyTsnrwnrRf8j7xzIf/yr1v9JuO6U2+NH91feCOvTe58N6wN1PI4DMbbsQCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kgvPZq6FgumdZwf+pZa95D2Q4nx0AYQdSQdiBRBB2IBGEHUgEYQcSQdiBRBSG3cymm9lKM3vJzDaa2U3Z8jvMbLuZrcu+5te+3SblHn8NDsRfQB2M5uIV/ZJudve1ZjZR0hozW5HV7nP3e2rXHoBqGc387Dsk7chu7zOzTZLyp0cB0JQO6zO7mZ0uabakVdmiG8zsRTNbYmaTc9bpNLNuM+vuU0+5bgFUbNRhN7PjJP1A0lfcfa+kBySdKWmWhrb83xhpPXdf7O4d7t7RpnHlOwZQkVGF3czaNBT0R9z9cUly953uPuDug5K+LWlO7doEUNZo9sabpAclbXL3e4ctnzbsbpdLyp+uE0DDjWZv/LmSrpG03szWZctul7TAzGZJcklbJV1Xg/4AVMlo9sb/VNJI58c+Vf12ANQKR9ABiSDsQCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kgrADiSDsQCIIO5AIwg4kgrADiSDsQCLqOmWzmf1c0pvDFk2R9E7dGjg8zdpbs/Yl0Vulqtnbae5+wkiFuob9I09u1u3uHQ1rINCsvTVrXxK9VapevfE2HkgEYQcS0eiwL27w80eatbdm7Uuit0rVpbeGfmYHUD+N3rIDqBPCDiSiIWE3s4vN7BUz22JmtzWihzxmttXM1mfTUHc3uJclZrbLzDYMW9ZuZivMbHP2fcQ59hrUW1NM4x1MM97Q167R05/X/TO7mbVKelXS70raJmm1pAXu/lJdG8lhZlsldbh7ww/AMLPzJe2X9G/u/qls2d2Sdrv7Xdl/lJPd/dYm6e0OSfsbPY13NlvRtOHTjEu6TNK1auBrF/R1lerwujViyz5H0hZ3f93deyU9KunSBvTR9Nz9GUm7D1l8qaSl2e2lGvpjqbuc3pqCu+9w97XZ7X2S3p9mvKGvXdBXXTQi7CdLemvYz9vUXPO9u6SnzWyNmXU2upkRTHX3HdnttyVNbWQzIyicxrueDplmvGleu0qmPy+LHXQfdZ67/4akSyRdn71dbUo+9BmsmcZORzWNd72MMM34Bxr52lU6/XlZjQj7dknTh/18SrasKbj79uz7LklPqPmmot75/gy62fddDe7nA800jfdI04yrCV67Rk5/3oiwr5Y0w8zOMLOxkq6WtLwBfXyEmU3IdpzIzCZIukjNNxX1ckkLs9sLJT3ZwF4+pFmm8c6bZlwNfu0aPv25u9f9S9J8De2Rf03SXzeih5y+flnSC9nXxkb3JmmZht7W9Wlo38YXJX1cUpekzZJ+LKm9iXp7WNJ6SS9qKFjTGtTbeRp6i/6ipHXZ1/xGv3ZBX3V53ThcFkgEO+iARBB2IBGEHUgEYQcSQdiBRBB2IBGEHUjE/wNJG/KiqyaaEgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "autoencodeur.eval()\n",
    "data_reconstructed = autoencodeur(example).cpu().reshape(28,28).detach().numpy()\n",
    "plt.imshow(data_reconstructed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
