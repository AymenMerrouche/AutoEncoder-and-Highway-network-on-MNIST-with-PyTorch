{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Question_1_3_SGD import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load, split and scale the data\n",
    "X, y = load_boston(return_X_y = True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y.reshape(-1,1), test_size=0.33, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "# we fit the scaler on the train\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "# data\n",
    "x_train_input = torch.from_numpy(X_train.astype('float32'))\n",
    "y_train_input = torch.from_numpy(y_train.astype('float32'))\n",
    "\n",
    "x_test_input = torch.from_numpy(X_test.astype('float32'))\n",
    "y_test_input = torch.from_numpy(y_test.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writer at :  2020-11-07_22h39.19\n"
     ]
    }
   ],
   "source": [
    "# writer tensorboard\n",
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%Hh%M.%S\")\n",
    "print(\"writer at : \",current_time)\n",
    "writer = SummaryWriter(f'./SGD/LOSS_{current_time}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 0: loss 573.4993896484375\n",
      "Itérations_test 0: loss 515.962646484375\n",
      "Itérations_train 1: loss 549.338134765625\n",
      "Itérations_test 1: loss 495.3435363769531\n",
      "Itérations_train 2: loss 526.6492309570312\n",
      "Itérations_test 2: loss 475.94976806640625\n",
      "Itérations_train 3: loss 505.2591247558594\n",
      "Itérations_test 3: loss 457.62762451171875\n",
      "Itérations_train 4: loss 485.02734375\n",
      "Itérations_test 4: loss 440.2538146972656\n",
      "Itérations_train 5: loss 465.8385925292969\n",
      "Itérations_test 5: loss 423.7307434082031\n",
      "Itérations_train 6: loss 447.5991516113281\n",
      "Itérations_test 6: loss 407.97821044921875\n",
      "Itérations_train 7: loss 430.230224609375\n",
      "Itérations_test 7: loss 392.9316711425781\n",
      "Itérations_train 8: loss 413.66436767578125\n",
      "Itérations_test 8: loss 378.53717041015625\n",
      "Itérations_train 9: loss 397.8460388183594\n",
      "Itérations_test 9: loss 364.75\n",
      "Itérations_train 10: loss 382.7256774902344\n",
      "Itérations_test 10: loss 351.5318908691406\n",
      "Itérations_train 11: loss 368.2607727050781\n",
      "Itérations_test 11: loss 338.8497619628906\n",
      "Itérations_train 12: loss 354.4139404296875\n",
      "Itérations_test 12: loss 326.67510986328125\n",
      "Itérations_train 13: loss 341.1513366699219\n",
      "Itérations_test 13: loss 314.98236083984375\n",
      "Itérations_train 14: loss 328.44317626953125\n",
      "Itérations_test 14: loss 303.7486572265625\n",
      "Itérations_train 15: loss 316.2616271972656\n",
      "Itérations_test 15: loss 292.9533386230469\n",
      "Itérations_train 16: loss 304.5817565917969\n",
      "Itérations_test 16: loss 282.57733154296875\n",
      "Itérations_train 17: loss 293.3797912597656\n",
      "Itérations_test 17: loss 272.6027526855469\n",
      "Itérations_train 18: loss 282.6346435546875\n",
      "Itérations_test 18: loss 263.0135192871094\n",
      "Itérations_train 19: loss 272.3255920410156\n",
      "Itérations_test 19: loss 253.79388427734375\n",
      "Itérations_train 20: loss 262.4340515136719\n",
      "Itérations_test 20: loss 244.9292755126953\n",
      "Itérations_train 21: loss 252.94207763671875\n",
      "Itérations_test 21: loss 236.40586853027344\n",
      "Itérations_train 22: loss 243.83255004882812\n",
      "Itérations_test 22: loss 228.21009826660156\n",
      "Itérations_train 23: loss 235.08958435058594\n",
      "Itérations_test 23: loss 220.32968139648438\n",
      "Itérations_train 24: loss 226.69775390625\n",
      "Itérations_test 24: loss 212.7523193359375\n",
      "Itérations_train 25: loss 218.64271545410156\n",
      "Itérations_test 25: loss 205.46649169921875\n",
      "Itérations_train 26: loss 210.9104461669922\n",
      "Itérations_test 26: loss 198.4610137939453\n",
      "Itérations_train 27: loss 203.4881134033203\n",
      "Itérations_test 27: loss 191.72511291503906\n",
      "Itérations_train 28: loss 196.3626708984375\n",
      "Itérations_test 28: loss 185.24851989746094\n",
      "Itérations_train 29: loss 189.5220947265625\n",
      "Itérations_test 29: loss 179.02145385742188\n",
      "Itérations_train 30: loss 182.95518493652344\n",
      "Itérations_test 30: loss 173.03421020507812\n",
      "Itérations_train 31: loss 176.650390625\n",
      "Itérations_test 31: loss 167.27761840820312\n",
      "Itérations_train 32: loss 170.59756469726562\n",
      "Itérations_test 32: loss 161.74290466308594\n",
      "Itérations_train 33: loss 164.7863311767578\n",
      "Itérations_test 33: loss 156.4215850830078\n",
      "Itérations_train 34: loss 159.20672607421875\n",
      "Itérations_test 34: loss 151.30538940429688\n",
      "Itérations_train 35: loss 153.84983825683594\n",
      "Itérations_test 35: loss 146.38645935058594\n",
      "Itérations_train 36: loss 148.70652770996094\n",
      "Itérations_test 36: loss 141.65733337402344\n",
      "Itérations_train 37: loss 143.76821899414062\n",
      "Itérations_test 37: loss 137.1104736328125\n",
      "Itérations_train 38: loss 139.0266571044922\n",
      "Itérations_test 38: loss 132.73907470703125\n",
      "Itérations_train 39: loss 134.4740447998047\n",
      "Itérations_test 39: loss 128.5362091064453\n",
      "Itérations_train 40: loss 130.10272216796875\n",
      "Itérations_test 40: loss 124.49549865722656\n",
      "Itérations_train 41: loss 125.9054183959961\n",
      "Itérations_test 41: loss 120.61065673828125\n",
      "Itérations_train 42: loss 121.87516021728516\n",
      "Itérations_test 42: loss 116.87547302246094\n",
      "Itérations_train 43: loss 118.00521087646484\n",
      "Itérations_test 43: loss 113.28451538085938\n",
      "Itérations_train 44: loss 114.28926849365234\n",
      "Itérations_test 44: loss 109.8319091796875\n",
      "Itérations_train 45: loss 110.72107696533203\n",
      "Itérations_test 45: loss 106.51243591308594\n",
      "Itérations_train 46: loss 107.2947998046875\n",
      "Itérations_test 46: loss 103.32089233398438\n",
      "Itérations_train 47: loss 104.00464630126953\n",
      "Itérations_test 47: loss 100.25228118896484\n",
      "Itérations_train 48: loss 100.84528350830078\n",
      "Itérations_test 48: loss 97.30196380615234\n",
      "Itérations_train 49: loss 97.81137084960938\n",
      "Itérations_test 49: loss 94.46519470214844\n",
      "Itérations_train 50: loss 94.89802551269531\n",
      "Itérations_test 50: loss 91.73771667480469\n",
      "Itérations_train 51: loss 92.10025787353516\n",
      "Itérations_test 51: loss 89.11519622802734\n",
      "Itérations_train 52: loss 89.41360473632812\n",
      "Itérations_test 52: loss 86.59358978271484\n",
      "Itérations_train 53: loss 86.83354187011719\n",
      "Itérations_test 53: loss 84.1689682006836\n",
      "Itérations_train 54: loss 84.3558349609375\n",
      "Itérations_test 54: loss 81.83760070800781\n",
      "Itérations_train 55: loss 81.97640991210938\n",
      "Itérations_test 55: loss 79.5958023071289\n",
      "Itérations_train 56: loss 79.6913070678711\n",
      "Itérations_test 56: loss 77.44020080566406\n",
      "Itérations_train 57: loss 77.49678802490234\n",
      "Itérations_test 57: loss 75.36736297607422\n",
      "Itérations_train 58: loss 75.38923645019531\n",
      "Itérations_test 58: loss 73.37416076660156\n",
      "Itérations_train 59: loss 73.36516571044922\n",
      "Itérations_test 59: loss 71.45738220214844\n",
      "Itérations_train 60: loss 71.42122650146484\n",
      "Itérations_test 60: loss 69.61414337158203\n",
      "Itérations_train 61: loss 69.55426788330078\n",
      "Itérations_test 61: loss 67.84158325195312\n",
      "Itérations_train 62: loss 67.7612075805664\n",
      "Itérations_test 62: loss 66.13697052001953\n",
      "Itérations_train 63: loss 66.03907775878906\n",
      "Itérations_test 63: loss 64.49761962890625\n",
      "Itérations_train 64: loss 64.38511657714844\n",
      "Itérations_test 64: loss 62.92104721069336\n",
      "Itérations_train 65: loss 62.79646682739258\n",
      "Itérations_test 65: loss 61.40476608276367\n",
      "Itérations_train 66: loss 61.270660400390625\n",
      "Itérations_test 66: loss 59.946502685546875\n",
      "Itérations_train 67: loss 59.805206298828125\n",
      "Itérations_test 67: loss 58.54393005371094\n",
      "Itérations_train 68: loss 58.39752197265625\n",
      "Itérations_test 68: loss 57.19493865966797\n",
      "Itérations_train 69: loss 57.04549026489258\n",
      "Itérations_test 69: loss 55.89746856689453\n",
      "Itérations_train 70: loss 55.7468147277832\n",
      "Itérations_test 70: loss 54.649444580078125\n",
      "Itérations_train 71: loss 54.499359130859375\n",
      "Itérations_test 71: loss 53.449058532714844\n",
      "Itérations_train 72: loss 53.30117416381836\n",
      "Itérations_test 72: loss 52.29434585571289\n",
      "Itérations_train 73: loss 52.15019607543945\n",
      "Itérations_test 73: loss 51.18361282348633\n",
      "Itérations_train 74: loss 51.044620513916016\n",
      "Itérations_test 74: loss 50.115142822265625\n",
      "Itérations_train 75: loss 49.982566833496094\n",
      "Itérations_test 75: loss 49.08726501464844\n",
      "Itérations_train 76: loss 48.96236801147461\n",
      "Itérations_test 76: loss 48.09848403930664\n",
      "Itérations_train 77: loss 47.982330322265625\n",
      "Itérations_test 77: loss 47.14719772338867\n",
      "Itérations_train 78: loss 47.040855407714844\n",
      "Itérations_test 78: loss 46.231998443603516\n",
      "Itérations_train 79: loss 46.13639831542969\n",
      "Itérations_test 79: loss 45.35150146484375\n",
      "Itérations_train 80: loss 45.267547607421875\n",
      "Itérations_test 80: loss 44.50432205200195\n",
      "Itérations_train 81: loss 44.43281173706055\n",
      "Itérations_test 81: loss 43.68919372558594\n",
      "Itérations_train 82: loss 43.63088607788086\n",
      "Itérations_test 82: loss 42.90488815307617\n",
      "Itérations_train 83: loss 42.8604850769043\n",
      "Itérations_test 83: loss 42.150203704833984\n",
      "Itérations_train 84: loss 42.120296478271484\n",
      "Itérations_test 84: loss 41.42399597167969\n",
      "Itérations_train 85: loss 41.40913009643555\n",
      "Itérations_test 85: loss 40.72514724731445\n",
      "Itérations_train 86: loss 40.725860595703125\n",
      "Itérations_test 86: loss 40.05264663696289\n",
      "Itérations_train 87: loss 40.06938934326172\n",
      "Itérations_test 87: loss 39.40544128417969\n",
      "Itérations_train 88: loss 39.4386100769043\n",
      "Itérations_test 88: loss 38.782527923583984\n",
      "Itérations_train 89: loss 38.832576751708984\n",
      "Itérations_test 89: loss 38.18302917480469\n",
      "Itérations_train 90: loss 38.250186920166016\n",
      "Itérations_test 90: loss 37.605979919433594\n",
      "Itérations_train 91: loss 37.69060516357422\n",
      "Itérations_test 91: loss 37.05055236816406\n",
      "Itérations_train 92: loss 37.152896881103516\n",
      "Itérations_test 92: loss 36.51594161987305\n",
      "Itérations_train 93: loss 36.636199951171875\n",
      "Itérations_test 93: loss 36.00126647949219\n",
      "Itérations_train 94: loss 36.13969421386719\n",
      "Itérations_test 94: loss 35.5058479309082\n",
      "Itérations_train 95: loss 35.662513732910156\n",
      "Itérations_test 95: loss 35.02886962890625\n",
      "Itérations_train 96: loss 35.203983306884766\n",
      "Itérations_test 96: loss 34.56965637207031\n",
      "Itérations_train 97: loss 34.76329040527344\n",
      "Itérations_test 97: loss 34.12752914428711\n",
      "Itérations_train 98: loss 34.339786529541016\n",
      "Itérations_test 98: loss 33.701805114746094\n",
      "Itérations_train 99: loss 33.9327392578125\n",
      "Itérations_test 99: loss 33.291893005371094\n",
      "Itérations_train 100: loss 33.541561126708984\n",
      "Itérations_test 100: loss 32.897159576416016\n",
      "Itérations_train 101: loss 33.165550231933594\n",
      "Itérations_test 101: loss 32.51698684692383\n",
      "Itérations_train 102: loss 32.804141998291016\n",
      "Itérations_test 102: loss 32.15089416503906\n",
      "Itérations_train 103: loss 32.45679473876953\n",
      "Itérations_test 103: loss 31.79827117919922\n",
      "Itérations_train 104: loss 32.12287139892578\n",
      "Itérations_test 104: loss 31.458629608154297\n",
      "Itérations_train 105: loss 31.80190658569336\n",
      "Itérations_test 105: loss 31.131492614746094\n",
      "Itérations_train 106: loss 31.49336814880371\n",
      "Itérations_test 106: loss 30.81634521484375\n",
      "Itérations_train 107: loss 31.196746826171875\n",
      "Itérations_test 107: loss 30.51274299621582\n",
      "Itérations_train 108: loss 30.91160774230957\n",
      "Itérations_test 108: loss 30.22024154663086\n",
      "Itérations_train 109: loss 30.637470245361328\n",
      "Itérations_test 109: loss 29.93842887878418\n",
      "Itérations_train 110: loss 30.373910903930664\n",
      "Itérations_test 110: loss 29.666885375976562\n",
      "Itérations_train 111: loss 30.12049102783203\n",
      "Itérations_test 111: loss 29.405183792114258\n",
      "Itérations_train 112: loss 29.876855850219727\n",
      "Itérations_test 112: loss 29.15302085876465\n",
      "Itérations_train 113: loss 29.6425838470459\n",
      "Itérations_test 113: loss 28.909969329833984\n",
      "Itérations_train 114: loss 29.41731834411621\n",
      "Itérations_test 114: loss 28.675729751586914\n",
      "Itérations_train 115: loss 29.20069122314453\n",
      "Itérations_test 115: loss 28.449920654296875\n",
      "Itérations_train 116: loss 28.992372512817383\n",
      "Itérations_test 116: loss 28.232254028320312\n",
      "Itérations_train 117: loss 28.792020797729492\n",
      "Itérations_test 117: loss 28.02239990234375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 118: loss 28.59934425354004\n",
      "Itérations_test 118: loss 27.820070266723633\n",
      "Itérations_train 119: loss 28.41402816772461\n",
      "Itérations_test 119: loss 27.624980926513672\n",
      "Itérations_train 120: loss 28.235776901245117\n",
      "Itérations_test 120: loss 27.436845779418945\n",
      "Itérations_train 121: loss 28.064334869384766\n",
      "Itérations_test 121: loss 27.255416870117188\n",
      "Itérations_train 122: loss 27.89939308166504\n",
      "Itérations_test 122: loss 27.080427169799805\n",
      "Itérations_train 123: loss 27.74074363708496\n",
      "Itérations_test 123: loss 26.911630630493164\n",
      "Itérations_train 124: loss 27.588109970092773\n",
      "Itérations_test 124: loss 26.748796463012695\n",
      "Itérations_train 125: loss 27.441259384155273\n",
      "Itérations_test 125: loss 26.591711044311523\n",
      "Itérations_train 126: loss 27.299962997436523\n",
      "Itérations_test 126: loss 26.440143585205078\n",
      "Itérations_train 127: loss 27.164033889770508\n",
      "Itérations_test 127: loss 26.29387855529785\n",
      "Itérations_train 128: loss 27.03322410583496\n",
      "Itérations_test 128: loss 26.152727127075195\n",
      "Itérations_train 129: loss 26.907323837280273\n",
      "Itérations_test 129: loss 26.01650619506836\n",
      "Itérations_train 130: loss 26.786170959472656\n",
      "Itérations_test 130: loss 25.884992599487305\n",
      "Itérations_train 131: loss 26.669559478759766\n",
      "Itérations_test 131: loss 25.758056640625\n",
      "Itérations_train 132: loss 26.557334899902344\n",
      "Itérations_test 132: loss 25.635509490966797\n",
      "Itérations_train 133: loss 26.449291229248047\n",
      "Itérations_test 133: loss 25.517160415649414\n",
      "Itérations_train 134: loss 26.345287322998047\n",
      "Itérations_test 134: loss 25.402875900268555\n",
      "Itérations_train 135: loss 26.245166778564453\n",
      "Itérations_test 135: loss 25.292495727539062\n",
      "Itérations_train 136: loss 26.148786544799805\n",
      "Itérations_test 136: loss 25.18588638305664\n",
      "Itérations_train 137: loss 26.055984497070312\n",
      "Itérations_test 137: loss 25.082895278930664\n",
      "Itérations_train 138: loss 25.966594696044922\n",
      "Itérations_test 138: loss 24.983381271362305\n",
      "Itérations_train 139: loss 25.880521774291992\n",
      "Itérations_test 139: loss 24.88722038269043\n",
      "Itérations_train 140: loss 25.79762077331543\n",
      "Itérations_test 140: loss 24.79430389404297\n",
      "Itérations_train 141: loss 25.71779441833496\n",
      "Itérations_test 141: loss 24.7044734954834\n",
      "Itérations_train 142: loss 25.640869140625\n",
      "Itérations_test 142: loss 24.617652893066406\n",
      "Itérations_train 143: loss 25.5667781829834\n",
      "Itérations_test 143: loss 24.533706665039062\n",
      "Itérations_train 144: loss 25.495393753051758\n",
      "Itérations_test 144: loss 24.452550888061523\n",
      "Itérations_train 145: loss 25.426624298095703\n",
      "Itérations_test 145: loss 24.37403678894043\n",
      "Itérations_train 146: loss 25.360340118408203\n",
      "Itérations_test 146: loss 24.298110961914062\n",
      "Itérations_train 147: loss 25.296464920043945\n",
      "Itérations_test 147: loss 24.224658966064453\n",
      "Itérations_train 148: loss 25.234905242919922\n",
      "Itérations_test 148: loss 24.153602600097656\n",
      "Itérations_train 149: loss 25.175559997558594\n",
      "Itérations_test 149: loss 24.084835052490234\n",
      "Itérations_train 150: loss 25.11836051940918\n",
      "Itérations_test 150: loss 24.01827621459961\n",
      "Itérations_train 151: loss 25.06318473815918\n",
      "Itérations_test 151: loss 23.953857421875\n",
      "Itérations_train 152: loss 25.010007858276367\n",
      "Itérations_test 152: loss 23.89148712158203\n",
      "Itérations_train 153: loss 24.958715438842773\n",
      "Itérations_test 153: loss 23.831106185913086\n",
      "Itérations_train 154: loss 24.90924644470215\n",
      "Itérations_test 154: loss 23.77261734008789\n",
      "Itérations_train 155: loss 24.861526489257812\n",
      "Itérations_test 155: loss 23.715965270996094\n",
      "Itérations_train 156: loss 24.815502166748047\n",
      "Itérations_test 156: loss 23.661075592041016\n",
      "Itérations_train 157: loss 24.77108383178711\n",
      "Itérations_test 157: loss 23.607913970947266\n",
      "Itérations_train 158: loss 24.72822380065918\n",
      "Itérations_test 158: loss 23.556373596191406\n",
      "Itérations_train 159: loss 24.68686294555664\n",
      "Itérations_test 159: loss 23.50641441345215\n",
      "Itérations_train 160: loss 24.646934509277344\n",
      "Itérations_test 160: loss 23.457988739013672\n",
      "Itérations_train 161: loss 24.6084041595459\n",
      "Itérations_test 161: loss 23.411039352416992\n",
      "Itérations_train 162: loss 24.571186065673828\n",
      "Itérations_test 162: loss 23.365493774414062\n",
      "Itérations_train 163: loss 24.53525161743164\n",
      "Itérations_test 163: loss 23.321321487426758\n",
      "Itérations_train 164: loss 24.500564575195312\n",
      "Itérations_test 164: loss 23.278459548950195\n",
      "Itérations_train 165: loss 24.467041015625\n",
      "Itérations_test 165: loss 23.236860275268555\n",
      "Itérations_train 166: loss 24.434648513793945\n",
      "Itérations_test 166: loss 23.196496963500977\n",
      "Itérations_train 167: loss 24.403352737426758\n",
      "Itérations_test 167: loss 23.15732192993164\n",
      "Itérations_train 168: loss 24.37312126159668\n",
      "Itérations_test 168: loss 23.1192626953125\n",
      "Itérations_train 169: loss 24.34388542175293\n",
      "Itérations_test 169: loss 23.082313537597656\n",
      "Itérations_train 170: loss 24.31562614440918\n",
      "Itérations_test 170: loss 23.046417236328125\n",
      "Itérations_train 171: loss 24.288307189941406\n",
      "Itérations_test 171: loss 23.01155662536621\n",
      "Itérations_train 172: loss 24.261892318725586\n",
      "Itérations_test 172: loss 22.977664947509766\n",
      "Itérations_train 173: loss 24.23633575439453\n",
      "Itérations_test 173: loss 22.944726943969727\n",
      "Itérations_train 174: loss 24.211620330810547\n",
      "Itérations_test 174: loss 22.912704467773438\n",
      "Itérations_train 175: loss 24.187685012817383\n",
      "Itérations_test 175: loss 22.881572723388672\n",
      "Itérations_train 176: loss 24.16454315185547\n",
      "Itérations_test 176: loss 22.851280212402344\n",
      "Itérations_train 177: loss 24.14213752746582\n",
      "Itérations_test 177: loss 22.821821212768555\n",
      "Itérations_train 178: loss 24.120426177978516\n",
      "Itérations_test 178: loss 22.79315948486328\n",
      "Itérations_train 179: loss 24.099445343017578\n",
      "Itérations_test 179: loss 22.765262603759766\n",
      "Itérations_train 180: loss 24.079099655151367\n",
      "Itérations_test 180: loss 22.73810577392578\n",
      "Itérations_train 181: loss 24.059375762939453\n",
      "Itérations_test 181: loss 22.711669921875\n",
      "Itérations_train 182: loss 24.0402774810791\n",
      "Itérations_test 182: loss 22.685914993286133\n",
      "Itérations_train 183: loss 24.02176856994629\n",
      "Itérations_test 183: loss 22.660825729370117\n",
      "Itérations_train 184: loss 24.00381088256836\n",
      "Itérations_test 184: loss 22.636388778686523\n",
      "Itérations_train 185: loss 23.98641586303711\n",
      "Itérations_test 185: loss 22.612552642822266\n",
      "Itérations_train 186: loss 23.969558715820312\n",
      "Itérations_test 186: loss 22.589353561401367\n",
      "Itérations_train 187: loss 23.95318031311035\n",
      "Itérations_test 187: loss 22.566707611083984\n",
      "Itérations_train 188: loss 23.937320709228516\n",
      "Itérations_test 188: loss 22.544641494750977\n",
      "Itérations_train 189: loss 23.921903610229492\n",
      "Itérations_test 189: loss 22.523096084594727\n",
      "Itérations_train 190: loss 23.906944274902344\n",
      "Itérations_test 190: loss 22.502084732055664\n",
      "Itérations_train 191: loss 23.89241600036621\n",
      "Itérations_test 191: loss 22.481590270996094\n",
      "Itérations_train 192: loss 23.87831687927246\n",
      "Itérations_test 192: loss 22.461565017700195\n",
      "Itérations_train 193: loss 23.864601135253906\n",
      "Itérations_test 193: loss 22.4420223236084\n",
      "Itérations_train 194: loss 23.851301193237305\n",
      "Itérations_test 194: loss 22.422935485839844\n",
      "Itérations_train 195: loss 23.83837127685547\n",
      "Itérations_test 195: loss 22.404294967651367\n",
      "Itérations_train 196: loss 23.82578468322754\n",
      "Itérations_test 196: loss 22.38607406616211\n",
      "Itérations_train 197: loss 23.813562393188477\n",
      "Itérations_test 197: loss 22.368284225463867\n",
      "Itérations_train 198: loss 23.801660537719727\n",
      "Itérations_test 198: loss 22.35087776184082\n",
      "Itérations_train 199: loss 23.790102005004883\n",
      "Itérations_test 199: loss 22.333858489990234\n",
      "Itérations_train 200: loss 23.778839111328125\n",
      "Itérations_test 200: loss 22.31722068786621\n",
      "Itérations_train 201: loss 23.767881393432617\n",
      "Itérations_test 201: loss 22.30095100402832\n",
      "Itérations_train 202: loss 23.757205963134766\n",
      "Itérations_test 202: loss 22.285024642944336\n",
      "Itérations_train 203: loss 23.746822357177734\n",
      "Itérations_test 203: loss 22.26944351196289\n",
      "Itérations_train 204: loss 23.7367000579834\n",
      "Itérations_test 204: loss 22.254182815551758\n",
      "Itérations_train 205: loss 23.726835250854492\n",
      "Itérations_test 205: loss 22.239246368408203\n",
      "Itérations_train 206: loss 23.71721649169922\n",
      "Itérations_test 206: loss 22.224613189697266\n",
      "Itérations_train 207: loss 23.70785140991211\n",
      "Itérations_test 207: loss 22.21028709411621\n",
      "Itérations_train 208: loss 23.698715209960938\n",
      "Itérations_test 208: loss 22.196239471435547\n",
      "Itérations_train 209: loss 23.689796447753906\n",
      "Itérations_test 209: loss 22.182477951049805\n",
      "Itérations_train 210: loss 23.681089401245117\n",
      "Itérations_test 210: loss 22.168994903564453\n",
      "Itérations_train 211: loss 23.672603607177734\n",
      "Itérations_test 211: loss 22.155778884887695\n",
      "Itérations_train 212: loss 23.664325714111328\n",
      "Itérations_test 212: loss 22.142797470092773\n",
      "Itérations_train 213: loss 23.65622901916504\n",
      "Itérations_test 213: loss 22.130090713500977\n",
      "Itérations_train 214: loss 23.648317337036133\n",
      "Itérations_test 214: loss 22.11760902404785\n",
      "Itérations_train 215: loss 23.64059829711914\n",
      "Itérations_test 215: loss 22.10537338256836\n",
      "Itérations_train 216: loss 23.633056640625\n",
      "Itérations_test 216: loss 22.093347549438477\n",
      "Itérations_train 217: loss 23.625673294067383\n",
      "Itérations_test 217: loss 22.081541061401367\n",
      "Itérations_train 218: loss 23.618467330932617\n",
      "Itérations_test 218: loss 22.069950103759766\n",
      "Itérations_train 219: loss 23.61140251159668\n",
      "Itérations_test 219: loss 22.058574676513672\n",
      "Itérations_train 220: loss 23.60450553894043\n",
      "Itérations_test 220: loss 22.04740333557129\n",
      "Itérations_train 221: loss 23.59775733947754\n",
      "Itérations_test 221: loss 22.03642463684082\n",
      "Itérations_train 222: loss 23.591142654418945\n",
      "Itérations_test 222: loss 22.025623321533203\n",
      "Itérations_train 223: loss 23.584667205810547\n",
      "Itérations_test 223: loss 22.015018463134766\n",
      "Itérations_train 224: loss 23.578332901000977\n",
      "Itérations_test 224: loss 22.004589080810547\n",
      "Itérations_train 225: loss 23.572114944458008\n",
      "Itérations_test 225: loss 21.99432945251465\n",
      "Itérations_train 226: loss 23.5660400390625\n",
      "Itérations_test 226: loss 21.98423194885254\n",
      "Itérations_train 227: loss 23.560073852539062\n",
      "Itérations_test 227: loss 21.974323272705078\n",
      "Itérations_train 228: loss 23.554218292236328\n",
      "Itérations_test 228: loss 21.964550018310547\n",
      "Itérations_train 229: loss 23.548477172851562\n",
      "Itérations_test 229: loss 21.954940795898438\n",
      "Itérations_train 230: loss 23.542856216430664\n",
      "Itérations_test 230: loss 21.94548225402832\n",
      "Itérations_train 231: loss 23.537343978881836\n",
      "Itérations_test 231: loss 21.936166763305664\n",
      "Itérations_train 232: loss 23.531919479370117\n",
      "Itérations_test 232: loss 21.927005767822266\n",
      "Itérations_train 233: loss 23.526609420776367\n",
      "Itérations_test 233: loss 21.91796875\n",
      "Itérations_train 234: loss 23.521387100219727\n",
      "Itérations_test 234: loss 21.90907859802246\n",
      "Itérations_train 235: loss 23.51625633239746\n",
      "Itérations_test 235: loss 21.900320053100586\n",
      "Itérations_train 236: loss 23.511213302612305\n",
      "Itérations_test 236: loss 21.891687393188477\n",
      "Itérations_train 237: loss 23.50627899169922\n",
      "Itérations_test 237: loss 21.883176803588867\n",
      "Itérations_train 238: loss 23.501420974731445\n",
      "Itérations_test 238: loss 21.874780654907227\n",
      "Itérations_train 239: loss 23.496639251708984\n",
      "Itérations_test 239: loss 21.86651611328125\n",
      "Itérations_train 240: loss 23.49193572998047\n",
      "Itérations_test 240: loss 21.85836410522461\n",
      "Itérations_train 241: loss 23.48732566833496\n",
      "Itérations_test 241: loss 21.850330352783203\n",
      "Itérations_train 242: loss 23.48275375366211\n",
      "Itérations_test 242: loss 21.8424015045166\n",
      "Itérations_train 243: loss 23.47829246520996\n",
      "Itérations_test 243: loss 21.834579467773438\n",
      "Itérations_train 244: loss 23.473888397216797\n",
      "Itérations_test 244: loss 21.826854705810547\n",
      "Itérations_train 245: loss 23.469558715820312\n",
      "Itérations_test 245: loss 21.819244384765625\n",
      "Itérations_train 246: loss 23.46529197692871\n",
      "Itérations_test 246: loss 21.811717987060547\n",
      "Itérations_train 247: loss 23.461105346679688\n",
      "Itérations_test 247: loss 21.80430793762207\n",
      "Itérations_train 248: loss 23.456951141357422\n",
      "Itérations_test 248: loss 21.79698371887207\n",
      "Itérations_train 249: loss 23.45288848876953\n",
      "Itérations_test 249: loss 21.789751052856445\n",
      "Itérations_train 250: loss 23.448862075805664\n",
      "Itérations_test 250: loss 21.782621383666992\n",
      "Itérations_train 251: loss 23.444910049438477\n",
      "Itérations_test 251: loss 21.775556564331055\n",
      "Itérations_train 252: loss 23.441015243530273\n",
      "Itérations_test 252: loss 21.768598556518555\n",
      "Itérations_train 253: loss 23.437158584594727\n",
      "Itérations_test 253: loss 21.761714935302734\n",
      "Itérations_train 254: loss 23.43336296081543\n",
      "Itérations_test 254: loss 21.754907608032227\n",
      "Itérations_train 255: loss 23.429624557495117\n",
      "Itérations_test 255: loss 21.748191833496094\n",
      "Itérations_train 256: loss 23.42594337463379\n",
      "Itérations_test 256: loss 21.74155044555664\n",
      "Itérations_train 257: loss 23.422292709350586\n",
      "Itérations_test 257: loss 21.7349910736084\n",
      "Itérations_train 258: loss 23.418703079223633\n",
      "Itérations_test 258: loss 21.7285099029541\n",
      "Itérations_train 259: loss 23.415170669555664\n",
      "Itérations_test 259: loss 21.72209358215332\n",
      "Itérations_train 260: loss 23.411657333374023\n",
      "Itérations_test 260: loss 21.715763092041016\n",
      "Itérations_train 261: loss 23.40821647644043\n",
      "Itérations_test 261: loss 21.70948600769043\n",
      "Itérations_train 262: loss 23.404804229736328\n",
      "Itérations_test 262: loss 21.703289031982422\n",
      "Itérations_train 263: loss 23.40143585205078\n",
      "Itérations_test 263: loss 21.697153091430664\n",
      "Itérations_train 264: loss 23.398109436035156\n",
      "Itérations_test 264: loss 21.691091537475586\n",
      "Itérations_train 265: loss 23.394834518432617\n",
      "Itérations_test 265: loss 21.68508529663086\n",
      "Itérations_train 266: loss 23.391592025756836\n",
      "Itérations_test 266: loss 21.67914581298828\n",
      "Itérations_train 267: loss 23.388376235961914\n",
      "Itérations_test 267: loss 21.673274993896484\n",
      "Itérations_train 268: loss 23.385215759277344\n",
      "Itérations_test 268: loss 21.667461395263672\n",
      "Itérations_train 269: loss 23.3820743560791\n",
      "Itérations_test 269: loss 21.66170310974121\n",
      "Itérations_train 270: loss 23.378995895385742\n",
      "Itérations_test 270: loss 21.6560115814209\n",
      "Itérations_train 271: loss 23.375930786132812\n",
      "Itérations_test 271: loss 21.650379180908203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 272: loss 23.372896194458008\n",
      "Itérations_test 272: loss 21.644794464111328\n",
      "Itérations_train 273: loss 23.369918823242188\n",
      "Itérations_test 273: loss 21.63927459716797\n",
      "Itérations_train 274: loss 23.366962432861328\n",
      "Itérations_test 274: loss 21.63380241394043\n",
      "Itérations_train 275: loss 23.364030838012695\n",
      "Itérations_test 275: loss 21.628385543823242\n",
      "Itérations_train 276: loss 23.361129760742188\n",
      "Itérations_test 276: loss 21.62302017211914\n",
      "Itérations_train 277: loss 23.35828399658203\n",
      "Itérations_test 277: loss 21.617719650268555\n",
      "Itérations_train 278: loss 23.355436325073242\n",
      "Itérations_test 278: loss 21.612445831298828\n",
      "Itérations_train 279: loss 23.352643966674805\n",
      "Itérations_test 279: loss 21.607242584228516\n",
      "Itérations_train 280: loss 23.349864959716797\n",
      "Itérations_test 280: loss 21.602083206176758\n",
      "Itérations_train 281: loss 23.347118377685547\n",
      "Itérations_test 281: loss 21.596969604492188\n",
      "Itérations_train 282: loss 23.344402313232422\n",
      "Itérations_test 282: loss 21.591890335083008\n",
      "Itérations_train 283: loss 23.341716766357422\n",
      "Itérations_test 283: loss 21.586885452270508\n",
      "Itérations_train 284: loss 23.33905792236328\n",
      "Itérations_test 284: loss 21.581907272338867\n",
      "Itérations_train 285: loss 23.3364315032959\n",
      "Itérations_test 285: loss 21.57697868347168\n",
      "Itérations_train 286: loss 23.333820343017578\n",
      "Itérations_test 286: loss 21.572093963623047\n",
      "Itérations_train 287: loss 23.331226348876953\n",
      "Itérations_test 287: loss 21.567249298095703\n",
      "Itérations_train 288: loss 23.32866096496582\n",
      "Itérations_test 288: loss 21.562456130981445\n",
      "Itérations_train 289: loss 23.326139450073242\n",
      "Itérations_test 289: loss 21.557701110839844\n",
      "Itérations_train 290: loss 23.32362937927246\n",
      "Itérations_test 290: loss 21.5529842376709\n",
      "Itérations_train 291: loss 23.321147918701172\n",
      "Itérations_test 291: loss 21.548309326171875\n",
      "Itérations_train 292: loss 23.31868553161621\n",
      "Itérations_test 292: loss 21.54368019104004\n",
      "Itérations_train 293: loss 23.31624984741211\n",
      "Itérations_test 293: loss 21.539087295532227\n",
      "Itérations_train 294: loss 23.31382942199707\n",
      "Itérations_test 294: loss 21.53452491760254\n",
      "Itérations_train 295: loss 23.311439514160156\n",
      "Itérations_test 295: loss 21.530014038085938\n",
      "Itérations_train 296: loss 23.309062957763672\n",
      "Itérations_test 296: loss 21.525527954101562\n",
      "Itérations_train 297: loss 23.306713104248047\n",
      "Itérations_test 297: loss 21.52109718322754\n",
      "Itérations_train 298: loss 23.304386138916016\n",
      "Itérations_test 298: loss 21.51668357849121\n",
      "Itérations_train 299: loss 23.302078247070312\n",
      "Itérations_test 299: loss 21.512317657470703\n",
      "Itérations_train 300: loss 23.299789428710938\n",
      "Itérations_test 300: loss 21.50797462463379\n",
      "Itérations_train 301: loss 23.297521591186523\n",
      "Itérations_test 301: loss 21.503690719604492\n",
      "Itérations_train 302: loss 23.295272827148438\n",
      "Itérations_test 302: loss 21.499420166015625\n",
      "Itérations_train 303: loss 23.293041229248047\n",
      "Itérations_test 303: loss 21.495180130004883\n",
      "Itérations_train 304: loss 23.290857315063477\n",
      "Itérations_test 304: loss 21.490985870361328\n",
      "Itérations_train 305: loss 23.28864097595215\n",
      "Itérations_test 305: loss 21.486831665039062\n",
      "Itérations_train 306: loss 23.28648567199707\n",
      "Itérations_test 306: loss 21.48270034790039\n",
      "Itérations_train 307: loss 23.284326553344727\n",
      "Itérations_test 307: loss 21.47860336303711\n",
      "Itérations_train 308: loss 23.28219223022461\n",
      "Itérations_test 308: loss 21.474529266357422\n",
      "Itérations_train 309: loss 23.280092239379883\n",
      "Itérations_test 309: loss 21.470489501953125\n",
      "Itérations_train 310: loss 23.277984619140625\n",
      "Itérations_test 310: loss 21.466495513916016\n",
      "Itérations_train 311: loss 23.275897979736328\n",
      "Itérations_test 311: loss 21.462528228759766\n",
      "Itérations_train 312: loss 23.273841857910156\n",
      "Itérations_test 312: loss 21.458574295043945\n",
      "Itérations_train 313: loss 23.27179527282715\n",
      "Itérations_test 313: loss 21.45467185974121\n",
      "Itérations_train 314: loss 23.269773483276367\n",
      "Itérations_test 314: loss 21.450786590576172\n",
      "Itérations_train 315: loss 23.267749786376953\n",
      "Itérations_test 315: loss 21.44692611694336\n",
      "Itérations_train 316: loss 23.265750885009766\n",
      "Itérations_test 316: loss 21.4431095123291\n",
      "Itérations_train 317: loss 23.263776779174805\n",
      "Itérations_test 317: loss 21.439311981201172\n",
      "Itérations_train 318: loss 23.261804580688477\n",
      "Itérations_test 318: loss 21.435537338256836\n",
      "Itérations_train 319: loss 23.259862899780273\n",
      "Itérations_test 319: loss 21.431800842285156\n",
      "Itérations_train 320: loss 23.257944107055664\n",
      "Itérations_test 320: loss 21.428085327148438\n",
      "Itérations_train 321: loss 23.25601577758789\n",
      "Itérations_test 321: loss 21.424392700195312\n",
      "Itérations_train 322: loss 23.25411605834961\n",
      "Itérations_test 322: loss 21.420738220214844\n",
      "Itérations_train 323: loss 23.252241134643555\n",
      "Itérations_test 323: loss 21.417104721069336\n",
      "Itérations_train 324: loss 23.250368118286133\n",
      "Itérations_test 324: loss 21.41349220275879\n",
      "Itérations_train 325: loss 23.24851417541504\n",
      "Itérations_test 325: loss 21.4099178314209\n",
      "Itérations_train 326: loss 23.246675491333008\n",
      "Itérations_test 326: loss 21.406360626220703\n",
      "Itérations_train 327: loss 23.244844436645508\n",
      "Itérations_test 327: loss 21.402822494506836\n",
      "Itérations_train 328: loss 23.243030548095703\n",
      "Itérations_test 328: loss 21.39931869506836\n",
      "Itérations_train 329: loss 23.24121856689453\n",
      "Itérations_test 329: loss 21.395851135253906\n",
      "Itérations_train 330: loss 23.239431381225586\n",
      "Itérations_test 330: loss 21.392383575439453\n",
      "Itérations_train 331: loss 23.237689971923828\n",
      "Itérations_test 331: loss 21.388954162597656\n",
      "Itérations_train 332: loss 23.235912322998047\n",
      "Itérations_test 332: loss 21.385540008544922\n",
      "Itérations_train 333: loss 23.234174728393555\n",
      "Itérations_test 333: loss 21.382156372070312\n",
      "Itérations_train 334: loss 23.232437133789062\n",
      "Itérations_test 334: loss 21.37879753112793\n",
      "Itérations_train 335: loss 23.23072624206543\n",
      "Itérations_test 335: loss 21.375455856323242\n",
      "Itérations_train 336: loss 23.229021072387695\n",
      "Itérations_test 336: loss 21.372146606445312\n",
      "Itérations_train 337: loss 23.227323532104492\n",
      "Itérations_test 337: loss 21.368846893310547\n",
      "Itérations_train 338: loss 23.22564125061035\n",
      "Itérations_test 338: loss 21.365571975708008\n",
      "Itérations_train 339: loss 23.223983764648438\n",
      "Itérations_test 339: loss 21.36233139038086\n",
      "Itérations_train 340: loss 23.222318649291992\n",
      "Itérations_test 340: loss 21.359098434448242\n",
      "Itérations_train 341: loss 23.22068977355957\n",
      "Itérations_test 341: loss 21.355892181396484\n",
      "Itérations_train 342: loss 23.21906852722168\n",
      "Itérations_test 342: loss 21.352710723876953\n",
      "Itérations_train 343: loss 23.21744728088379\n",
      "Itérations_test 343: loss 21.349546432495117\n",
      "Itérations_train 344: loss 23.21584129333496\n",
      "Itérations_test 344: loss 21.346405029296875\n",
      "Itérations_train 345: loss 23.214248657226562\n",
      "Itérations_test 345: loss 21.343292236328125\n",
      "Itérations_train 346: loss 23.21266746520996\n",
      "Itérations_test 346: loss 21.34018325805664\n",
      "Itérations_train 347: loss 23.21111488342285\n",
      "Itérations_test 347: loss 21.33710289001465\n",
      "Itérations_train 348: loss 23.209545135498047\n",
      "Itérations_test 348: loss 21.334043502807617\n",
      "Itérations_train 349: loss 23.20800018310547\n",
      "Itérations_test 349: loss 21.331008911132812\n",
      "Itérations_train 350: loss 23.206470489501953\n",
      "Itérations_test 350: loss 21.3279972076416\n",
      "Itérations_train 351: loss 23.204954147338867\n",
      "Itérations_test 351: loss 21.324989318847656\n",
      "Itérations_train 352: loss 23.203441619873047\n",
      "Itérations_test 352: loss 21.322006225585938\n",
      "Itérations_train 353: loss 23.20194435119629\n",
      "Itérations_test 353: loss 21.319046020507812\n",
      "Itérations_train 354: loss 23.200450897216797\n",
      "Itérations_test 354: loss 21.316102981567383\n",
      "Itérations_train 355: loss 23.198970794677734\n",
      "Itérations_test 355: loss 21.313186645507812\n",
      "Itérations_train 356: loss 23.197498321533203\n",
      "Itérations_test 356: loss 21.310277938842773\n",
      "Itérations_train 357: loss 23.196044921875\n",
      "Itérations_test 357: loss 21.30738639831543\n",
      "Itérations_train 358: loss 23.194591522216797\n",
      "Itérations_test 358: loss 21.30452537536621\n",
      "Itérations_train 359: loss 23.193159103393555\n",
      "Itérations_test 359: loss 21.30167007446289\n",
      "Itérations_train 360: loss 23.191734313964844\n",
      "Itérations_test 360: loss 21.29884147644043\n",
      "Itérations_train 361: loss 23.190326690673828\n",
      "Itérations_test 361: loss 21.296037673950195\n",
      "Itérations_train 362: loss 23.188913345336914\n",
      "Itérations_test 362: loss 21.29323959350586\n",
      "Itérations_train 363: loss 23.18752098083496\n",
      "Itérations_test 363: loss 21.29045867919922\n",
      "Itérations_train 364: loss 23.18614959716797\n",
      "Itérations_test 364: loss 21.287704467773438\n",
      "Itérations_train 365: loss 23.18476676940918\n",
      "Itérations_test 365: loss 21.284963607788086\n",
      "Itérations_train 366: loss 23.18340301513672\n",
      "Itérations_test 366: loss 21.282228469848633\n",
      "Itérations_train 367: loss 23.182048797607422\n",
      "Itérations_test 367: loss 21.27952766418457\n",
      "Itérations_train 368: loss 23.180707931518555\n",
      "Itérations_test 368: loss 21.276824951171875\n",
      "Itérations_train 369: loss 23.17937469482422\n",
      "Itérations_test 369: loss 21.274154663085938\n",
      "Itérations_train 370: loss 23.178043365478516\n",
      "Itérations_test 370: loss 21.271509170532227\n",
      "Itérations_train 371: loss 23.17673110961914\n",
      "Itérations_test 371: loss 21.268856048583984\n",
      "Itérations_train 372: loss 23.1754207611084\n",
      "Itérations_test 372: loss 21.266231536865234\n",
      "Itérations_train 373: loss 23.17413330078125\n",
      "Itérations_test 373: loss 21.26361656188965\n",
      "Itérations_train 374: loss 23.172832489013672\n",
      "Itérations_test 374: loss 21.261030197143555\n",
      "Itérations_train 375: loss 23.171560287475586\n",
      "Itérations_test 375: loss 21.258445739746094\n",
      "Itérations_train 376: loss 23.170278549194336\n",
      "Itérations_test 376: loss 21.255889892578125\n",
      "Itérations_train 377: loss 23.169025421142578\n",
      "Itérations_test 377: loss 21.253334045410156\n",
      "Itérations_train 378: loss 23.16777801513672\n",
      "Itérations_test 378: loss 21.250808715820312\n",
      "Itérations_train 379: loss 23.166545867919922\n",
      "Itérations_test 379: loss 21.248292922973633\n",
      "Itérations_train 380: loss 23.165306091308594\n",
      "Itérations_test 380: loss 21.245786666870117\n",
      "Itérations_train 381: loss 23.164079666137695\n",
      "Itérations_test 381: loss 21.243305206298828\n",
      "Itérations_train 382: loss 23.162864685058594\n",
      "Itérations_test 382: loss 21.24083137512207\n",
      "Itérations_train 383: loss 23.161659240722656\n",
      "Itérations_test 383: loss 21.238378524780273\n",
      "Itérations_train 384: loss 23.160442352294922\n",
      "Itérations_test 384: loss 21.235929489135742\n",
      "Itérations_train 385: loss 23.15926170349121\n",
      "Itérations_test 385: loss 21.23350715637207\n",
      "Itérations_train 386: loss 23.158077239990234\n",
      "Itérations_test 386: loss 21.231101989746094\n",
      "Itérations_train 387: loss 23.156909942626953\n",
      "Itérations_test 387: loss 21.22870445251465\n",
      "Itérations_train 388: loss 23.155736923217773\n",
      "Itérations_test 388: loss 21.226318359375\n",
      "Itérations_train 389: loss 23.154586791992188\n",
      "Itérations_test 389: loss 21.223949432373047\n",
      "Itérations_train 390: loss 23.153423309326172\n",
      "Itérations_test 390: loss 21.221593856811523\n",
      "Itérations_train 391: loss 23.152299880981445\n",
      "Itérations_test 391: loss 21.21925163269043\n",
      "Itérations_train 392: loss 23.151166915893555\n",
      "Itérations_test 392: loss 21.2169246673584\n",
      "Itérations_train 393: loss 23.150041580200195\n",
      "Itérations_test 393: loss 21.214616775512695\n",
      "Itérations_train 394: loss 23.14892578125\n",
      "Itérations_test 394: loss 21.212312698364258\n",
      "Itérations_train 395: loss 23.147817611694336\n",
      "Itérations_test 395: loss 21.210023880004883\n",
      "Itérations_train 396: loss 23.14672088623047\n",
      "Itérations_test 396: loss 21.207752227783203\n",
      "Itérations_train 397: loss 23.14561653137207\n",
      "Itérations_test 397: loss 21.20549774169922\n",
      "Itérations_train 398: loss 23.144535064697266\n",
      "Itérations_test 398: loss 21.203243255615234\n",
      "Itérations_train 399: loss 23.143455505371094\n",
      "Itérations_test 399: loss 21.20101547241211\n",
      "Itérations_train 400: loss 23.142391204833984\n",
      "Itérations_test 400: loss 21.198789596557617\n",
      "Itérations_train 401: loss 23.141319274902344\n",
      "Itérations_test 401: loss 21.19658851623535\n",
      "Itérations_train 402: loss 23.14026641845703\n",
      "Itérations_test 402: loss 21.194385528564453\n",
      "Itérations_train 403: loss 23.13922882080078\n",
      "Itérations_test 403: loss 21.192203521728516\n",
      "Itérations_train 404: loss 23.138179779052734\n",
      "Itérations_test 404: loss 21.190046310424805\n",
      "Itérations_train 405: loss 23.137157440185547\n",
      "Itérations_test 405: loss 21.187883377075195\n",
      "Itérations_train 406: loss 23.136112213134766\n",
      "Itérations_test 406: loss 21.185733795166016\n",
      "Itérations_train 407: loss 23.13511085510254\n",
      "Itérations_test 407: loss 21.183612823486328\n",
      "Itérations_train 408: loss 23.13408660888672\n",
      "Itérations_test 408: loss 21.18148422241211\n",
      "Itérations_train 409: loss 23.133079528808594\n",
      "Itérations_test 409: loss 21.179378509521484\n",
      "Itérations_train 410: loss 23.132081985473633\n",
      "Itérations_test 410: loss 21.17728614807129\n",
      "Itérations_train 411: loss 23.131086349487305\n",
      "Itérations_test 411: loss 21.175199508666992\n",
      "Itérations_train 412: loss 23.13011360168457\n",
      "Itérations_test 412: loss 21.17313003540039\n",
      "Itérations_train 413: loss 23.12912940979004\n",
      "Itérations_test 413: loss 21.17107582092285\n",
      "Itérations_train 414: loss 23.128164291381836\n",
      "Itérations_test 414: loss 21.169015884399414\n",
      "Itérations_train 415: loss 23.127197265625\n",
      "Itérations_test 415: loss 21.166990280151367\n",
      "Itérations_train 416: loss 23.126235961914062\n",
      "Itérations_test 416: loss 21.164960861206055\n",
      "Itérations_train 417: loss 23.125295639038086\n",
      "Itérations_test 417: loss 21.162952423095703\n",
      "Itérations_train 418: loss 23.12435531616211\n",
      "Itérations_test 418: loss 21.160951614379883\n",
      "Itérations_train 419: loss 23.123409271240234\n",
      "Itérations_test 419: loss 21.15895652770996\n",
      "Itérations_train 420: loss 23.122472763061523\n",
      "Itérations_test 420: loss 21.156980514526367\n",
      "Itérations_train 421: loss 23.121549606323242\n",
      "Itérations_test 421: loss 21.155014038085938\n",
      "Itérations_train 422: loss 23.12063217163086\n",
      "Itérations_test 422: loss 21.153057098388672\n",
      "Itérations_train 423: loss 23.119728088378906\n",
      "Itérations_test 423: loss 21.15110969543457\n",
      "Itérations_train 424: loss 23.118825912475586\n",
      "Itérations_test 424: loss 21.149169921875\n",
      "Itérations_train 425: loss 23.117919921875\n",
      "Itérations_test 425: loss 21.147249221801758\n",
      "Itérations_train 426: loss 23.117042541503906\n",
      "Itérations_test 426: loss 21.145339965820312\n",
      "Itérations_train 427: loss 23.116146087646484\n",
      "Itérations_test 427: loss 21.143434524536133\n",
      "Itérations_train 428: loss 23.115257263183594\n",
      "Itérations_test 428: loss 21.141542434692383\n",
      "Itérations_train 429: loss 23.114395141601562\n",
      "Itérations_test 429: loss 21.139652252197266\n",
      "Itérations_train 430: loss 23.113515853881836\n",
      "Itérations_test 430: loss 21.137784957885742\n",
      "Itérations_train 431: loss 23.112653732299805\n",
      "Itérations_test 431: loss 21.135923385620117\n",
      "Itérations_train 432: loss 23.111801147460938\n",
      "Itérations_test 432: loss 21.134077072143555\n",
      "Itérations_train 433: loss 23.11095428466797\n",
      "Itérations_test 433: loss 21.132234573364258\n",
      "Itérations_train 434: loss 23.110109329223633\n",
      "Itérations_test 434: loss 21.13040542602539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 435: loss 23.109277725219727\n",
      "Itérations_test 435: loss 21.12858009338379\n",
      "Itérations_train 436: loss 23.108440399169922\n",
      "Itérations_test 436: loss 21.126773834228516\n",
      "Itérations_train 437: loss 23.107606887817383\n",
      "Itérations_test 437: loss 21.12497329711914\n",
      "Itérations_train 438: loss 23.10679054260254\n",
      "Itérations_test 438: loss 21.12318992614746\n",
      "Itérations_train 439: loss 23.10597038269043\n",
      "Itérations_test 439: loss 21.121400833129883\n",
      "Itérations_train 440: loss 23.105165481567383\n",
      "Itérations_test 440: loss 21.1196346282959\n",
      "Itérations_train 441: loss 23.104368209838867\n",
      "Itérations_test 441: loss 21.11787223815918\n",
      "Itérations_train 442: loss 23.103565216064453\n",
      "Itérations_test 442: loss 21.116125106811523\n",
      "Itérations_train 443: loss 23.102764129638672\n",
      "Itérations_test 443: loss 21.114391326904297\n",
      "Itérations_train 444: loss 23.10199546813965\n",
      "Itérations_test 444: loss 21.11264991760254\n",
      "Itérations_train 445: loss 23.101194381713867\n",
      "Itérations_test 445: loss 21.110923767089844\n",
      "Itérations_train 446: loss 23.10042953491211\n",
      "Itérations_test 446: loss 21.109214782714844\n",
      "Itérations_train 447: loss 23.09965705871582\n",
      "Itérations_test 447: loss 21.107511520385742\n",
      "Itérations_train 448: loss 23.098892211914062\n",
      "Itérations_test 448: loss 21.10582160949707\n",
      "Itérations_train 449: loss 23.098127365112305\n",
      "Itérations_test 449: loss 21.104135513305664\n",
      "Itérations_train 450: loss 23.097362518310547\n",
      "Itérations_test 450: loss 21.102458953857422\n",
      "Itérations_train 451: loss 23.09661293029785\n",
      "Itérations_test 451: loss 21.100788116455078\n",
      "Itérations_train 452: loss 23.09588050842285\n",
      "Itérations_test 452: loss 21.099140167236328\n",
      "Itérations_train 453: loss 23.095144271850586\n",
      "Itérations_test 453: loss 21.097482681274414\n",
      "Itérations_train 454: loss 23.094402313232422\n",
      "Itérations_test 454: loss 21.095842361450195\n",
      "Itérations_train 455: loss 23.09366226196289\n",
      "Itérations_test 455: loss 21.094213485717773\n",
      "Itérations_train 456: loss 23.092952728271484\n",
      "Itérations_test 456: loss 21.092592239379883\n",
      "Itérations_train 457: loss 23.09222984313965\n",
      "Itérations_test 457: loss 21.09097671508789\n",
      "Itérations_train 458: loss 23.091503143310547\n",
      "Itérations_test 458: loss 21.08936882019043\n",
      "Itérations_train 459: loss 23.09079933166504\n",
      "Itérations_test 459: loss 21.087770462036133\n",
      "Itérations_train 460: loss 23.090085983276367\n",
      "Itérations_test 460: loss 21.086181640625\n",
      "Itérations_train 461: loss 23.089401245117188\n",
      "Itérations_test 461: loss 21.08461570739746\n",
      "Itérations_train 462: loss 23.088695526123047\n",
      "Itérations_test 462: loss 21.083040237426758\n",
      "Itérations_train 463: loss 23.087995529174805\n",
      "Itérations_test 463: loss 21.081478118896484\n",
      "Itérations_train 464: loss 23.087326049804688\n",
      "Itérations_test 464: loss 21.079919815063477\n",
      "Itérations_train 465: loss 23.086637496948242\n",
      "Itérations_test 465: loss 21.078380584716797\n",
      "Itérations_train 466: loss 23.085956573486328\n",
      "Itérations_test 466: loss 21.07683753967285\n",
      "Itérations_train 467: loss 23.085298538208008\n",
      "Itérations_test 467: loss 21.07530975341797\n",
      "Itérations_train 468: loss 23.08462142944336\n",
      "Itérations_test 468: loss 21.07378387451172\n",
      "Itérations_train 469: loss 23.083961486816406\n",
      "Itérations_test 469: loss 21.072284698486328\n",
      "Itérations_train 470: loss 23.083288192749023\n",
      "Itérations_test 470: loss 21.070764541625977\n",
      "Itérations_train 471: loss 23.0826473236084\n",
      "Itérations_test 471: loss 21.069276809692383\n",
      "Itérations_train 472: loss 23.081987380981445\n",
      "Itérations_test 472: loss 21.067790985107422\n",
      "Itérations_train 473: loss 23.08135414123535\n",
      "Itérations_test 473: loss 21.06630516052246\n",
      "Itérations_train 474: loss 23.080720901489258\n",
      "Itérations_test 474: loss 21.064828872680664\n",
      "Itérations_train 475: loss 23.080068588256836\n",
      "Itérations_test 475: loss 21.063369750976562\n",
      "Itérations_train 476: loss 23.079442977905273\n",
      "Itérations_test 476: loss 21.061904907226562\n",
      "Itérations_train 477: loss 23.078821182250977\n",
      "Itérations_test 477: loss 21.06046485900879\n",
      "Itérations_train 478: loss 23.078197479248047\n",
      "Itérations_test 478: loss 21.059019088745117\n",
      "Itérations_train 479: loss 23.07758331298828\n",
      "Itérations_test 479: loss 21.057586669921875\n",
      "Itérations_train 480: loss 23.076953887939453\n",
      "Itérations_test 480: loss 21.0561580657959\n",
      "Itérations_train 481: loss 23.07634925842285\n",
      "Itérations_test 481: loss 21.05473518371582\n",
      "Itérations_train 482: loss 23.07574462890625\n",
      "Itérations_test 482: loss 21.053327560424805\n",
      "Itérations_train 483: loss 23.075145721435547\n",
      "Itérations_test 483: loss 21.051923751831055\n",
      "Itérations_train 484: loss 23.07454490661621\n",
      "Itérations_test 484: loss 21.05052375793457\n",
      "Itérations_train 485: loss 23.07396125793457\n",
      "Itérations_test 485: loss 21.049129486083984\n",
      "Itérations_train 486: loss 23.0733585357666\n",
      "Itérations_test 486: loss 21.047752380371094\n",
      "Itérations_train 487: loss 23.07277488708496\n",
      "Itérations_test 487: loss 21.04637336730957\n",
      "Itérations_train 488: loss 23.07219696044922\n",
      "Itérations_test 488: loss 21.04500961303711\n",
      "Itérations_train 489: loss 23.071603775024414\n",
      "Itérations_test 489: loss 21.043651580810547\n",
      "Itérations_train 490: loss 23.0710391998291\n",
      "Itérations_test 490: loss 21.042293548583984\n",
      "Itérations_train 491: loss 23.07047462463379\n",
      "Itérations_test 491: loss 21.040943145751953\n",
      "Itérations_train 492: loss 23.069904327392578\n",
      "Itérations_test 492: loss 21.03960609436035\n",
      "Itérations_train 493: loss 23.069334030151367\n",
      "Itérations_test 493: loss 21.038265228271484\n",
      "Itérations_train 494: loss 23.06878089904785\n",
      "Itérations_test 494: loss 21.036937713623047\n",
      "Itérations_train 495: loss 23.068227767944336\n",
      "Itérations_test 495: loss 21.03563117980957\n",
      "Itérations_train 496: loss 23.06766700744629\n",
      "Itérations_test 496: loss 21.034317016601562\n",
      "Itérations_train 497: loss 23.0671329498291\n",
      "Itérations_test 497: loss 21.033008575439453\n",
      "Itérations_train 498: loss 23.06657600402832\n",
      "Itérations_test 498: loss 21.031715393066406\n",
      "Itérations_train 499: loss 23.066038131713867\n",
      "Itérations_test 499: loss 21.030412673950195\n",
      "Itérations_train 500: loss 23.065513610839844\n",
      "Itérations_test 500: loss 21.029132843017578\n",
      "Itérations_train 501: loss 23.064964294433594\n",
      "Itérations_test 501: loss 21.027851104736328\n",
      "Itérations_train 502: loss 23.064451217651367\n",
      "Itérations_test 502: loss 21.02657699584961\n",
      "Itérations_train 503: loss 23.063932418823242\n",
      "Itérations_test 503: loss 21.02531623840332\n",
      "Itérations_train 504: loss 23.063411712646484\n",
      "Itérations_test 504: loss 21.024057388305664\n",
      "Itérations_train 505: loss 23.062883377075195\n",
      "Itérations_test 505: loss 21.022808074951172\n",
      "Itérations_train 506: loss 23.062360763549805\n",
      "Itérations_test 506: loss 21.021560668945312\n",
      "Itérations_train 507: loss 23.06185531616211\n",
      "Itérations_test 507: loss 21.020320892333984\n",
      "Itérations_train 508: loss 23.061344146728516\n",
      "Itérations_test 508: loss 21.019075393676758\n",
      "Itérations_train 509: loss 23.06084442138672\n",
      "Itérations_test 509: loss 21.017860412597656\n",
      "Itérations_train 510: loss 23.060352325439453\n",
      "Itérations_test 510: loss 21.016645431518555\n",
      "Itérations_train 511: loss 23.059858322143555\n",
      "Itérations_test 511: loss 21.015424728393555\n",
      "Itérations_train 512: loss 23.059349060058594\n",
      "Itérations_test 512: loss 21.014217376708984\n",
      "Itérations_train 513: loss 23.058866500854492\n",
      "Itérations_test 513: loss 21.013015747070312\n",
      "Itérations_train 514: loss 23.058372497558594\n",
      "Itérations_test 514: loss 21.011812210083008\n",
      "Itérations_train 515: loss 23.05790138244629\n",
      "Itérations_test 515: loss 21.0106258392334\n",
      "Itérations_train 516: loss 23.05742645263672\n",
      "Itérations_test 516: loss 21.009443283081055\n",
      "Itérations_train 517: loss 23.05693244934082\n",
      "Itérations_test 517: loss 21.008270263671875\n",
      "Itérations_train 518: loss 23.05646324157715\n",
      "Itérations_test 518: loss 21.007095336914062\n",
      "Itérations_train 519: loss 23.055986404418945\n",
      "Itérations_test 519: loss 21.00592803955078\n",
      "Itérations_train 520: loss 23.05552864074707\n",
      "Itérations_test 520: loss 21.004764556884766\n",
      "Itérations_train 521: loss 23.0550594329834\n",
      "Itérations_test 521: loss 21.00361442565918\n",
      "Itérations_train 522: loss 23.05460548400879\n",
      "Itérations_test 522: loss 21.00246810913086\n",
      "Itérations_train 523: loss 23.05414390563965\n",
      "Itérations_test 523: loss 21.00132179260254\n",
      "Itérations_train 524: loss 23.05367088317871\n",
      "Itérations_test 524: loss 21.000192642211914\n",
      "Itérations_train 525: loss 23.053220748901367\n",
      "Itérations_test 525: loss 20.999059677124023\n",
      "Itérations_train 526: loss 23.05278968811035\n",
      "Itérations_test 526: loss 20.997941970825195\n",
      "Itérations_train 527: loss 23.052337646484375\n",
      "Itérations_test 527: loss 20.996810913085938\n",
      "Itérations_train 528: loss 23.051902770996094\n",
      "Itérations_test 528: loss 20.995698928833008\n",
      "Itérations_train 529: loss 23.051454544067383\n",
      "Itérations_test 529: loss 20.994596481323242\n",
      "Itérations_train 530: loss 23.051015853881836\n",
      "Itérations_test 530: loss 20.99349021911621\n",
      "Itérations_train 531: loss 23.050586700439453\n",
      "Itérations_test 531: loss 20.992385864257812\n",
      "Itérations_train 532: loss 23.05014419555664\n",
      "Itérations_test 532: loss 20.991300582885742\n",
      "Itérations_train 533: loss 23.049715042114258\n",
      "Itérations_test 533: loss 20.990209579467773\n",
      "Itérations_train 534: loss 23.049297332763672\n",
      "Itérations_test 534: loss 20.989133834838867\n",
      "Itérations_train 535: loss 23.048871994018555\n",
      "Itérations_test 535: loss 20.988056182861328\n",
      "Itérations_train 536: loss 23.04845428466797\n",
      "Itérations_test 536: loss 20.98699188232422\n",
      "Itérations_train 537: loss 23.048036575317383\n",
      "Itérations_test 537: loss 20.98592758178711\n",
      "Itérations_train 538: loss 23.047622680664062\n",
      "Itérations_test 538: loss 20.9848575592041\n",
      "Itérations_train 539: loss 23.047208786010742\n",
      "Itérations_test 539: loss 20.98381233215332\n",
      "Itérations_train 540: loss 23.046798706054688\n",
      "Itérations_test 540: loss 20.982765197753906\n",
      "Itérations_train 541: loss 23.046401977539062\n",
      "Itérations_test 541: loss 20.981718063354492\n",
      "Itérations_train 542: loss 23.04599952697754\n",
      "Itérations_test 542: loss 20.98068618774414\n",
      "Itérations_train 543: loss 23.04559326171875\n",
      "Itérations_test 543: loss 20.97964859008789\n",
      "Itérations_train 544: loss 23.045190811157227\n",
      "Itérations_test 544: loss 20.978614807128906\n",
      "Itérations_train 545: loss 23.044797897338867\n",
      "Itérations_test 545: loss 20.97759437561035\n",
      "Itérations_train 546: loss 23.04440689086914\n",
      "Itérations_test 546: loss 20.976581573486328\n",
      "Itérations_train 547: loss 23.044025421142578\n",
      "Itérations_test 547: loss 20.97557258605957\n",
      "Itérations_train 548: loss 23.043628692626953\n",
      "Itérations_test 548: loss 20.97455406188965\n",
      "Itérations_train 549: loss 23.043249130249023\n",
      "Itérations_test 549: loss 20.97355842590332\n",
      "Itérations_train 550: loss 23.042871475219727\n",
      "Itérations_test 550: loss 20.97256088256836\n",
      "Itérations_train 551: loss 23.042482376098633\n",
      "Itérations_test 551: loss 20.971567153930664\n",
      "Itérations_train 552: loss 23.042104721069336\n",
      "Itérations_test 552: loss 20.970569610595703\n",
      "Itérations_train 553: loss 23.041738510131836\n",
      "Itérations_test 553: loss 20.969600677490234\n",
      "Itérations_train 554: loss 23.041364669799805\n",
      "Itérations_test 554: loss 20.96862030029297\n",
      "Itérations_train 555: loss 23.040987014770508\n",
      "Itérations_test 555: loss 20.967641830444336\n",
      "Itérations_train 556: loss 23.04062843322754\n",
      "Itérations_test 556: loss 20.966678619384766\n",
      "Itérations_train 557: loss 23.040271759033203\n",
      "Itérations_test 557: loss 20.965717315673828\n",
      "Itérations_train 558: loss 23.039913177490234\n",
      "Itérations_test 558: loss 20.964757919311523\n",
      "Itérations_train 559: loss 23.039546966552734\n",
      "Itérations_test 559: loss 20.96379852294922\n",
      "Itérations_train 560: loss 23.03919219970703\n",
      "Itérations_test 560: loss 20.962854385375977\n",
      "Itérations_train 561: loss 23.038829803466797\n",
      "Itérations_test 561: loss 20.961902618408203\n",
      "Itérations_train 562: loss 23.038484573364258\n",
      "Itérations_test 562: loss 20.960968017578125\n",
      "Itérations_train 563: loss 23.03814125061035\n",
      "Itérations_test 563: loss 20.960033416748047\n",
      "Itérations_train 564: loss 23.03778839111328\n",
      "Itérations_test 564: loss 20.959096908569336\n",
      "Itérations_train 565: loss 23.03743553161621\n",
      "Itérations_test 565: loss 20.95816993713379\n",
      "Itérations_train 566: loss 23.037099838256836\n",
      "Itérations_test 566: loss 20.95724868774414\n",
      "Itérations_train 567: loss 23.036758422851562\n",
      "Itérations_test 567: loss 20.956336975097656\n",
      "Itérations_train 568: loss 23.036418914794922\n",
      "Itérations_test 568: loss 20.955429077148438\n",
      "Itérations_train 569: loss 23.036073684692383\n",
      "Itérations_test 569: loss 20.95450782775879\n",
      "Itérations_train 570: loss 23.035764694213867\n",
      "Itérations_test 570: loss 20.953617095947266\n",
      "Itérations_train 571: loss 23.035429000854492\n",
      "Itérations_test 571: loss 20.95271110534668\n",
      "Itérations_train 572: loss 23.03510284423828\n",
      "Itérations_test 572: loss 20.951820373535156\n",
      "Itérations_train 573: loss 23.03476905822754\n",
      "Itérations_test 573: loss 20.950929641723633\n",
      "Itérations_train 574: loss 23.034446716308594\n",
      "Itérations_test 574: loss 20.950040817260742\n",
      "Itérations_train 575: loss 23.03412437438965\n",
      "Itérations_test 575: loss 20.94916343688965\n",
      "Itérations_train 576: loss 23.03380584716797\n",
      "Itérations_test 576: loss 20.94828987121582\n",
      "Itérations_train 577: loss 23.033483505249023\n",
      "Itérations_test 577: loss 20.947406768798828\n",
      "Itérations_train 578: loss 23.033170700073242\n",
      "Itérations_test 578: loss 20.946542739868164\n",
      "Itérations_train 579: loss 23.032855987548828\n",
      "Itérations_test 579: loss 20.945676803588867\n",
      "Itérations_train 580: loss 23.03253746032715\n",
      "Itérations_test 580: loss 20.94481658935547\n",
      "Itérations_train 581: loss 23.032222747802734\n",
      "Itérations_test 581: loss 20.94395637512207\n",
      "Itérations_train 582: loss 23.031917572021484\n",
      "Itérations_test 582: loss 20.943103790283203\n",
      "Itérations_train 583: loss 23.031606674194336\n",
      "Itérations_test 583: loss 20.942262649536133\n",
      "Itérations_train 584: loss 23.03130531311035\n",
      "Itérations_test 584: loss 20.941421508789062\n",
      "Itérations_train 585: loss 23.031017303466797\n",
      "Itérations_test 585: loss 20.940570831298828\n",
      "Itérations_train 586: loss 23.03070831298828\n",
      "Itérations_test 586: loss 20.939735412597656\n",
      "Itérations_train 587: loss 23.030405044555664\n",
      "Itérations_test 587: loss 20.93890380859375\n",
      "Itérations_train 588: loss 23.03011131286621\n",
      "Itérations_test 588: loss 20.938077926635742\n",
      "Itérations_train 589: loss 23.029821395874023\n",
      "Itérations_test 589: loss 20.9372501373291\n",
      "Itérations_train 590: loss 23.029531478881836\n",
      "Itérations_test 590: loss 20.93644142150879\n",
      "Itérations_train 591: loss 23.02924156188965\n",
      "Itérations_test 591: loss 20.93562126159668\n",
      "Itérations_train 592: loss 23.02895736694336\n",
      "Itérations_test 592: loss 20.9348087310791\n",
      "Itérations_train 593: loss 23.02865982055664\n",
      "Itérations_test 593: loss 20.933998107910156\n",
      "Itérations_train 594: loss 23.02837371826172\n",
      "Itérations_test 594: loss 20.933197021484375\n",
      "Itérations_train 595: loss 23.028099060058594\n",
      "Itérations_test 595: loss 20.932392120361328\n",
      "Itérations_train 596: loss 23.027828216552734\n",
      "Itérations_test 596: loss 20.931594848632812\n",
      "Itérations_train 597: loss 23.027542114257812\n",
      "Itérations_test 597: loss 20.930805206298828\n",
      "Itérations_train 598: loss 23.02726936340332\n",
      "Itérations_test 598: loss 20.930011749267578\n",
      "Itérations_train 599: loss 23.02699851989746\n",
      "Itérations_test 599: loss 20.92923355102539\n",
      "Itérations_train 600: loss 23.026721954345703\n",
      "Itérations_test 600: loss 20.928447723388672\n",
      "Itérations_train 601: loss 23.026464462280273\n",
      "Itérations_test 601: loss 20.92766761779785\n",
      "Itérations_train 602: loss 23.02618980407715\n",
      "Itérations_test 602: loss 20.926897048950195\n",
      "Itérations_train 603: loss 23.025911331176758\n",
      "Itérations_test 603: loss 20.926132202148438\n",
      "Itérations_train 604: loss 23.02566146850586\n",
      "Itérations_test 604: loss 20.925355911254883\n",
      "Itérations_train 605: loss 23.025386810302734\n",
      "Itérations_test 605: loss 20.924596786499023\n",
      "Itérations_train 606: loss 23.025121688842773\n",
      "Itérations_test 606: loss 20.923837661743164\n",
      "Itérations_train 607: loss 23.024858474731445\n",
      "Itérations_test 607: loss 20.92308235168457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 608: loss 23.024599075317383\n",
      "Itérations_test 608: loss 20.92232894897461\n",
      "Itérations_train 609: loss 23.02435302734375\n",
      "Itérations_test 609: loss 20.92158317565918\n",
      "Itérations_train 610: loss 23.024099349975586\n",
      "Itérations_test 610: loss 20.92083740234375\n",
      "Itérations_train 611: loss 23.023847579956055\n",
      "Itérations_test 611: loss 20.92009735107422\n",
      "Itérations_train 612: loss 23.023590087890625\n",
      "Itérations_test 612: loss 20.919349670410156\n",
      "Itérations_train 613: loss 23.023340225219727\n",
      "Itérations_test 613: loss 20.918622970581055\n",
      "Itérations_train 614: loss 23.023096084594727\n",
      "Itérations_test 614: loss 20.91788673400879\n",
      "Itérations_train 615: loss 23.022842407226562\n",
      "Itérations_test 615: loss 20.917165756225586\n",
      "Itérations_train 616: loss 23.02260398864746\n",
      "Itérations_test 616: loss 20.91643524169922\n",
      "Itérations_train 617: loss 23.022348403930664\n",
      "Itérations_test 617: loss 20.91572380065918\n",
      "Itérations_train 618: loss 23.022109985351562\n",
      "Itérations_test 618: loss 20.914995193481445\n",
      "Itérations_train 619: loss 23.021883010864258\n",
      "Itérations_test 619: loss 20.914287567138672\n",
      "Itérations_train 620: loss 23.021629333496094\n",
      "Itérations_test 620: loss 20.913572311401367\n",
      "Itérations_train 621: loss 23.021394729614258\n",
      "Itérations_test 621: loss 20.912872314453125\n",
      "Itérations_train 622: loss 23.021160125732422\n",
      "Itérations_test 622: loss 20.912158966064453\n",
      "Itérations_train 623: loss 23.02092170715332\n",
      "Itérations_test 623: loss 20.911462783813477\n",
      "Itérations_train 624: loss 23.020706176757812\n",
      "Itérations_test 624: loss 20.9107608795166\n",
      "Itérations_train 625: loss 23.020471572875977\n",
      "Itérations_test 625: loss 20.910070419311523\n",
      "Itérations_train 626: loss 23.020233154296875\n",
      "Itérations_test 626: loss 20.909381866455078\n",
      "Itérations_train 627: loss 23.020009994506836\n",
      "Itérations_test 627: loss 20.90869140625\n",
      "Itérations_train 628: loss 23.019784927368164\n",
      "Itérations_test 628: loss 20.908008575439453\n",
      "Itérations_train 629: loss 23.019554138183594\n",
      "Itérations_test 629: loss 20.907316207885742\n",
      "Itérations_train 630: loss 23.01933479309082\n",
      "Itérations_test 630: loss 20.90665626525879\n",
      "Itérations_train 631: loss 23.019115447998047\n",
      "Itérations_test 631: loss 20.905969619750977\n",
      "Itérations_train 632: loss 23.018905639648438\n",
      "Itérations_test 632: loss 20.90530014038086\n",
      "Itérations_train 633: loss 23.018678665161133\n",
      "Itérations_test 633: loss 20.904634475708008\n",
      "Itérations_train 634: loss 23.018461227416992\n",
      "Itérations_test 634: loss 20.90396499633789\n",
      "Itérations_train 635: loss 23.018238067626953\n",
      "Itérations_test 635: loss 20.903301239013672\n",
      "Itérations_train 636: loss 23.01803970336914\n",
      "Itérations_test 636: loss 20.902645111083984\n",
      "Itérations_train 637: loss 23.017820358276367\n",
      "Itérations_test 637: loss 20.901992797851562\n",
      "Itérations_train 638: loss 23.017610549926758\n",
      "Itérations_test 638: loss 20.901329040527344\n",
      "Itérations_train 639: loss 23.017406463623047\n",
      "Itérations_test 639: loss 20.90068244934082\n",
      "Itérations_train 640: loss 23.017196655273438\n",
      "Itérations_test 640: loss 20.900033950805664\n",
      "Itérations_train 641: loss 23.01698112487793\n",
      "Itérations_test 641: loss 20.899396896362305\n",
      "Itérations_train 642: loss 23.016769409179688\n",
      "Itérations_test 642: loss 20.898752212524414\n",
      "Itérations_train 643: loss 23.01657485961914\n",
      "Itérations_test 643: loss 20.898115158081055\n",
      "Itérations_train 644: loss 23.01636505126953\n",
      "Itérations_test 644: loss 20.89747428894043\n",
      "Itérations_train 645: loss 23.016170501708984\n",
      "Itérations_test 645: loss 20.896841049194336\n",
      "Itérations_train 646: loss 23.015972137451172\n",
      "Itérations_test 646: loss 20.896215438842773\n",
      "Itérations_train 647: loss 23.01576805114746\n",
      "Itérations_test 647: loss 20.89559555053711\n",
      "Itérations_train 648: loss 23.01555633544922\n",
      "Itérations_test 648: loss 20.894969940185547\n",
      "Itérations_train 649: loss 23.015371322631836\n",
      "Itérations_test 649: loss 20.894344329833984\n",
      "Itérations_train 650: loss 23.01518440246582\n",
      "Itérations_test 650: loss 20.89373207092285\n",
      "Itérations_train 651: loss 23.014986038208008\n",
      "Itérations_test 651: loss 20.893110275268555\n",
      "Itérations_train 652: loss 23.014785766601562\n",
      "Itérations_test 652: loss 20.89250946044922\n",
      "Itérations_train 653: loss 23.01459312438965\n",
      "Itérations_test 653: loss 20.89188575744629\n",
      "Itérations_train 654: loss 23.014404296875\n",
      "Itérations_test 654: loss 20.891292572021484\n",
      "Itérations_train 655: loss 23.014209747314453\n",
      "Itérations_test 655: loss 20.89068031311035\n",
      "Itérations_train 656: loss 23.014034271240234\n",
      "Itérations_test 656: loss 20.890092849731445\n",
      "Itérations_train 657: loss 23.013835906982422\n",
      "Itérations_test 657: loss 20.889482498168945\n",
      "Itérations_train 658: loss 23.013652801513672\n",
      "Itérations_test 658: loss 20.888893127441406\n",
      "Itérations_train 659: loss 23.013473510742188\n",
      "Itérations_test 659: loss 20.888296127319336\n",
      "Itérations_train 660: loss 23.01329231262207\n",
      "Itérations_test 660: loss 20.887712478637695\n",
      "Itérations_train 661: loss 23.013113021850586\n",
      "Itérations_test 661: loss 20.887121200561523\n",
      "Itérations_train 662: loss 23.012929916381836\n",
      "Itérations_test 662: loss 20.886547088623047\n",
      "Itérations_train 663: loss 23.012746810913086\n",
      "Itérations_test 663: loss 20.885955810546875\n",
      "Itérations_train 664: loss 23.012575149536133\n",
      "Itérations_test 664: loss 20.8853759765625\n",
      "Itérations_train 665: loss 23.01239776611328\n",
      "Itérations_test 665: loss 20.884790420532227\n",
      "Itérations_train 666: loss 23.012226104736328\n",
      "Itérations_test 666: loss 20.884233474731445\n",
      "Itérations_train 667: loss 23.012039184570312\n",
      "Itérations_test 667: loss 20.88365364074707\n",
      "Itérations_train 668: loss 23.01188087463379\n",
      "Itérations_test 668: loss 20.883089065551758\n",
      "Itérations_train 669: loss 23.01169776916504\n",
      "Itérations_test 669: loss 20.88252067565918\n",
      "Itérations_train 670: loss 23.01152992248535\n",
      "Itérations_test 670: loss 20.881954193115234\n",
      "Itérations_train 671: loss 23.011367797851562\n",
      "Itérations_test 671: loss 20.881397247314453\n",
      "Itérations_train 672: loss 23.011192321777344\n",
      "Itérations_test 672: loss 20.88083839416504\n",
      "Itérations_train 673: loss 23.01102066040039\n",
      "Itérations_test 673: loss 20.880287170410156\n",
      "Itérations_train 674: loss 23.0108585357666\n",
      "Itérations_test 674: loss 20.879728317260742\n",
      "Itérations_train 675: loss 23.01068687438965\n",
      "Itérations_test 675: loss 20.879180908203125\n",
      "Itérations_train 676: loss 23.010540008544922\n",
      "Itérations_test 676: loss 20.878633499145508\n",
      "Itérations_train 677: loss 23.01036262512207\n",
      "Itérations_test 677: loss 20.87807846069336\n",
      "Itérations_train 678: loss 23.01020622253418\n",
      "Itérations_test 678: loss 20.87754249572754\n",
      "Itérations_train 679: loss 23.01004409790039\n",
      "Itérations_test 679: loss 20.876998901367188\n",
      "Itérations_train 680: loss 23.009885787963867\n",
      "Itérations_test 680: loss 20.876462936401367\n",
      "Itérations_train 681: loss 23.00972557067871\n",
      "Itérations_test 681: loss 20.87592124938965\n",
      "Itérations_train 682: loss 23.009567260742188\n",
      "Itérations_test 682: loss 20.875391006469727\n",
      "Itérations_train 683: loss 23.009410858154297\n",
      "Itérations_test 683: loss 20.874858856201172\n",
      "Itérations_train 684: loss 23.009252548217773\n",
      "Itérations_test 684: loss 20.87433624267578\n",
      "Itérations_train 685: loss 23.009092330932617\n",
      "Itérations_test 685: loss 20.873802185058594\n",
      "Itérations_train 686: loss 23.00894546508789\n",
      "Itérations_test 686: loss 20.8732852935791\n",
      "Itérations_train 687: loss 23.008804321289062\n",
      "Itérations_test 687: loss 20.872766494750977\n",
      "Itérations_train 688: loss 23.00863265991211\n",
      "Itérations_test 688: loss 20.872234344482422\n",
      "Itérations_train 689: loss 23.008480072021484\n",
      "Itérations_test 689: loss 20.871726989746094\n",
      "Itérations_train 690: loss 23.008342742919922\n",
      "Itérations_test 690: loss 20.871200561523438\n",
      "Itérations_train 691: loss 23.008182525634766\n",
      "Itérations_test 691: loss 20.870689392089844\n",
      "Itérations_train 692: loss 23.00804901123047\n",
      "Itérations_test 692: loss 20.87018585205078\n",
      "Itérations_train 693: loss 23.007898330688477\n",
      "Itérations_test 693: loss 20.869680404663086\n",
      "Itérations_train 694: loss 23.00775718688965\n",
      "Itérations_test 694: loss 20.869157791137695\n",
      "Itérations_train 695: loss 23.007614135742188\n",
      "Itérations_test 695: loss 20.86866569519043\n",
      "Itérations_train 696: loss 23.007463455200195\n",
      "Itérations_test 696: loss 20.868173599243164\n",
      "Itérations_train 697: loss 23.007320404052734\n",
      "Itérations_test 697: loss 20.8676700592041\n",
      "Itérations_train 698: loss 23.007169723510742\n",
      "Itérations_test 698: loss 20.867176055908203\n",
      "Itérations_train 699: loss 23.007030487060547\n",
      "Itérations_test 699: loss 20.866674423217773\n",
      "Itérations_train 700: loss 23.006895065307617\n",
      "Itérations_test 700: loss 20.866178512573242\n",
      "Itérations_train 701: loss 23.006755828857422\n",
      "Itérations_test 701: loss 20.865692138671875\n",
      "Itérations_train 702: loss 23.00662612915039\n",
      "Itérations_test 702: loss 20.86520767211914\n",
      "Itérations_train 703: loss 23.006481170654297\n",
      "Itérations_test 703: loss 20.864715576171875\n",
      "Itérations_train 704: loss 23.006338119506836\n",
      "Itérations_test 704: loss 20.864233016967773\n",
      "Itérations_train 705: loss 23.006208419799805\n",
      "Itérations_test 705: loss 20.86375617980957\n",
      "Itérations_train 706: loss 23.006072998046875\n",
      "Itérations_test 706: loss 20.8632755279541\n",
      "Itérations_train 707: loss 23.005945205688477\n",
      "Itérations_test 707: loss 20.862796783447266\n",
      "Itérations_train 708: loss 23.00579071044922\n",
      "Itérations_test 708: loss 20.862327575683594\n",
      "Itérations_train 709: loss 23.00566291809082\n",
      "Itérations_test 709: loss 20.861846923828125\n",
      "Itérations_train 710: loss 23.005544662475586\n",
      "Itérations_test 710: loss 20.861379623413086\n",
      "Itérations_train 711: loss 23.005403518676758\n",
      "Itérations_test 711: loss 20.86090660095215\n",
      "Itérations_train 712: loss 23.005281448364258\n",
      "Itérations_test 712: loss 20.86044692993164\n",
      "Itérations_train 713: loss 23.00515365600586\n",
      "Itérations_test 713: loss 20.859975814819336\n",
      "Itérations_train 714: loss 23.00503158569336\n",
      "Itérations_test 714: loss 20.859519958496094\n",
      "Itérations_train 715: loss 23.00490379333496\n",
      "Itérations_test 715: loss 20.859052658081055\n",
      "Itérations_train 716: loss 23.00476837158203\n",
      "Itérations_test 716: loss 20.85858917236328\n",
      "Itérations_train 717: loss 23.004640579223633\n",
      "Itérations_test 717: loss 20.858139038085938\n",
      "Itérations_train 718: loss 23.00452995300293\n",
      "Itérations_test 718: loss 20.857683181762695\n",
      "Itérations_train 719: loss 23.00439453125\n",
      "Itérations_test 719: loss 20.85723304748535\n",
      "Itérations_train 720: loss 23.004283905029297\n",
      "Itérations_test 720: loss 20.85677719116211\n",
      "Itérations_train 721: loss 23.0041561126709\n",
      "Itérations_test 721: loss 20.856334686279297\n",
      "Itérations_train 722: loss 23.004032135009766\n",
      "Itérations_test 722: loss 20.85588264465332\n",
      "Itérations_train 723: loss 23.003917694091797\n",
      "Itérations_test 723: loss 20.855443954467773\n",
      "Itérations_train 724: loss 23.00379753112793\n",
      "Itérations_test 724: loss 20.854999542236328\n",
      "Itérations_train 725: loss 23.003671646118164\n",
      "Itérations_test 725: loss 20.85455894470215\n",
      "Itérations_train 726: loss 23.003543853759766\n",
      "Itérations_test 726: loss 20.85411834716797\n",
      "Itérations_train 727: loss 23.00342559814453\n",
      "Itérations_test 727: loss 20.853683471679688\n",
      "Itérations_train 728: loss 23.003314971923828\n",
      "Itérations_test 728: loss 20.85323715209961\n",
      "Itérations_train 729: loss 23.003206253051758\n",
      "Itérations_test 729: loss 20.852815628051758\n",
      "Itérations_train 730: loss 23.00309181213379\n",
      "Itérations_test 730: loss 20.85237693786621\n",
      "Itérations_train 731: loss 23.002979278564453\n",
      "Itérations_test 731: loss 20.851945877075195\n",
      "Itérations_train 732: loss 23.002853393554688\n",
      "Itérations_test 732: loss 20.85152244567871\n",
      "Itérations_train 733: loss 23.002750396728516\n",
      "Itérations_test 733: loss 20.851106643676758\n",
      "Itérations_train 734: loss 23.002634048461914\n",
      "Itérations_test 734: loss 20.850671768188477\n",
      "Itérations_train 735: loss 23.002519607543945\n",
      "Itérations_test 735: loss 20.850244522094727\n",
      "Itérations_train 736: loss 23.002410888671875\n",
      "Itérations_test 736: loss 20.849828720092773\n",
      "Itérations_train 737: loss 23.002304077148438\n",
      "Itérations_test 737: loss 20.84941291809082\n",
      "Itérations_train 738: loss 23.002185821533203\n",
      "Itérations_test 738: loss 20.848995208740234\n",
      "Itérations_train 739: loss 23.002073287963867\n",
      "Itérations_test 739: loss 20.84857940673828\n",
      "Itérations_train 740: loss 23.001970291137695\n",
      "Itérations_test 740: loss 20.848163604736328\n",
      "Itérations_train 741: loss 23.00187110900879\n",
      "Itérations_test 741: loss 20.84775733947754\n",
      "Itérations_train 742: loss 23.00176429748535\n",
      "Itérations_test 742: loss 20.847341537475586\n",
      "Itérations_train 743: loss 23.001651763916016\n",
      "Itérations_test 743: loss 20.84693145751953\n",
      "Itérations_train 744: loss 23.00155258178711\n",
      "Itérations_test 744: loss 20.846529006958008\n",
      "Itérations_train 745: loss 23.001441955566406\n",
      "Itérations_test 745: loss 20.846128463745117\n",
      "Itérations_train 746: loss 23.001354217529297\n",
      "Itérations_test 746: loss 20.845722198486328\n",
      "Itérations_train 747: loss 23.001237869262695\n",
      "Itérations_test 747: loss 20.845325469970703\n",
      "Itérations_train 748: loss 23.00113868713379\n",
      "Itérations_test 748: loss 20.844926834106445\n",
      "Itérations_train 749: loss 23.00103187561035\n",
      "Itérations_test 749: loss 20.844528198242188\n",
      "Itérations_train 750: loss 23.000938415527344\n",
      "Itérations_test 750: loss 20.84412956237793\n",
      "Itérations_train 751: loss 23.000837326049805\n",
      "Itérations_test 751: loss 20.843734741210938\n",
      "Itérations_train 752: loss 23.000730514526367\n",
      "Itérations_test 752: loss 20.84334373474121\n",
      "Itérations_train 753: loss 23.00065040588379\n",
      "Itérations_test 753: loss 20.842954635620117\n",
      "Itérations_train 754: loss 23.00053596496582\n",
      "Itérations_test 754: loss 20.842561721801758\n",
      "Itérations_train 755: loss 23.000444412231445\n",
      "Itérations_test 755: loss 20.842182159423828\n",
      "Itérations_train 756: loss 23.000341415405273\n",
      "Itérations_test 756: loss 20.841796875\n",
      "Itérations_train 757: loss 23.0002498626709\n",
      "Itérations_test 757: loss 20.841407775878906\n",
      "Itérations_train 758: loss 23.000158309936523\n",
      "Itérations_test 758: loss 20.841026306152344\n",
      "Itérations_train 759: loss 23.00006103515625\n",
      "Itérations_test 759: loss 20.840646743774414\n",
      "Itérations_train 760: loss 22.99995994567871\n",
      "Itérations_test 760: loss 20.84026336669922\n",
      "Itérations_train 761: loss 22.99986457824707\n",
      "Itérations_test 761: loss 20.83988380432129\n",
      "Itérations_train 762: loss 22.999773025512695\n",
      "Itérations_test 762: loss 20.839506149291992\n",
      "Itérations_train 763: loss 22.999685287475586\n",
      "Itérations_test 763: loss 20.839139938354492\n",
      "Itérations_train 764: loss 22.999584197998047\n",
      "Itérations_test 764: loss 20.838756561279297\n",
      "Itérations_train 765: loss 22.99948501586914\n",
      "Itérations_test 765: loss 20.838382720947266\n",
      "Itérations_train 766: loss 22.999406814575195\n",
      "Itérations_test 766: loss 20.8380184173584\n",
      "Itérations_train 767: loss 22.99932289123535\n",
      "Itérations_test 767: loss 20.83765411376953\n",
      "Itérations_train 768: loss 22.999231338500977\n",
      "Itérations_test 768: loss 20.8372859954834\n",
      "Itérations_train 769: loss 22.9991397857666\n",
      "Itérations_test 769: loss 20.83692169189453\n",
      "Itérations_train 770: loss 22.99905014038086\n",
      "Itérations_test 770: loss 20.83656120300293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 771: loss 22.99895668029785\n",
      "Itérations_test 771: loss 20.8361873626709\n",
      "Itérations_train 772: loss 22.99886131286621\n",
      "Itérations_test 772: loss 20.835844039916992\n",
      "Itérations_train 773: loss 22.998798370361328\n",
      "Itérations_test 773: loss 20.835472106933594\n",
      "Itérations_train 774: loss 22.998703002929688\n",
      "Itérations_test 774: loss 20.83512306213379\n",
      "Itérations_train 775: loss 22.998615264892578\n",
      "Itérations_test 775: loss 20.834768295288086\n",
      "Itérations_train 776: loss 22.998531341552734\n",
      "Itérations_test 776: loss 20.834413528442383\n",
      "Itérations_train 777: loss 22.998449325561523\n",
      "Itérations_test 777: loss 20.83405876159668\n",
      "Itérations_train 778: loss 22.998361587524414\n",
      "Itérations_test 778: loss 20.833707809448242\n",
      "Itérations_train 779: loss 22.998281478881836\n",
      "Itérations_test 779: loss 20.833362579345703\n",
      "Itérations_train 780: loss 22.99818992614746\n",
      "Itérations_test 780: loss 20.833003997802734\n",
      "Itérations_train 781: loss 22.99810218811035\n",
      "Itérations_test 781: loss 20.832658767700195\n",
      "Itérations_train 782: loss 22.99802017211914\n",
      "Itérations_test 782: loss 20.832311630249023\n",
      "Itérations_train 783: loss 22.997936248779297\n",
      "Itérations_test 783: loss 20.831972122192383\n",
      "Itérations_train 784: loss 22.99787712097168\n",
      "Itérations_test 784: loss 20.831626892089844\n",
      "Itérations_train 785: loss 22.99778175354004\n",
      "Itérations_test 785: loss 20.831289291381836\n",
      "Itérations_train 786: loss 22.997705459594727\n",
      "Itérations_test 786: loss 20.830947875976562\n",
      "Itérations_train 787: loss 22.997621536254883\n",
      "Itérations_test 787: loss 20.830608367919922\n",
      "Itérations_train 788: loss 22.99755096435547\n",
      "Itérations_test 788: loss 20.830276489257812\n",
      "Itérations_train 789: loss 22.99746322631836\n",
      "Itérations_test 789: loss 20.82993507385254\n",
      "Itérations_train 790: loss 22.997400283813477\n",
      "Itérations_test 790: loss 20.82959747314453\n",
      "Itérations_train 791: loss 22.997316360473633\n",
      "Itérations_test 791: loss 20.829265594482422\n",
      "Itérations_train 792: loss 22.997249603271484\n",
      "Itérations_test 792: loss 20.82893180847168\n",
      "Itérations_train 793: loss 22.997156143188477\n",
      "Itérations_test 793: loss 20.828603744506836\n",
      "Itérations_train 794: loss 22.997081756591797\n",
      "Itérations_test 794: loss 20.828271865844727\n",
      "Itérations_train 795: loss 22.99700927734375\n",
      "Itérations_test 795: loss 20.82795524597168\n",
      "Itérations_train 796: loss 22.996938705444336\n",
      "Itérations_test 796: loss 20.82762336730957\n",
      "Itérations_train 797: loss 22.996854782104492\n",
      "Itérations_test 797: loss 20.827302932739258\n",
      "Itérations_train 798: loss 22.99679183959961\n",
      "Itérations_test 798: loss 20.826976776123047\n",
      "Itérations_train 799: loss 22.996715545654297\n",
      "Itérations_test 799: loss 20.82666015625\n",
      "Itérations_train 800: loss 22.996641159057617\n",
      "Itérations_test 800: loss 20.826337814331055\n",
      "Itérations_train 801: loss 22.99656867980957\n",
      "Itérations_test 801: loss 20.82601547241211\n",
      "Itérations_train 802: loss 22.996501922607422\n",
      "Itérations_test 802: loss 20.82570457458496\n",
      "Itérations_train 803: loss 22.996437072753906\n",
      "Itérations_test 803: loss 20.825382232666016\n",
      "Itérations_train 804: loss 22.99635887145996\n",
      "Itérations_test 804: loss 20.825069427490234\n",
      "Itérations_train 805: loss 22.996278762817383\n",
      "Itérations_test 805: loss 20.824748992919922\n",
      "Itérations_train 806: loss 22.996219635009766\n",
      "Itérations_test 806: loss 20.82443618774414\n",
      "Itérations_train 807: loss 22.996150970458984\n",
      "Itérations_test 807: loss 20.82412338256836\n",
      "Itérations_train 808: loss 22.996076583862305\n",
      "Itérations_test 808: loss 20.823822021484375\n",
      "Itérations_train 809: loss 22.996013641357422\n",
      "Itérations_test 809: loss 20.82349967956543\n",
      "Itérations_train 810: loss 22.99594497680664\n",
      "Itérations_test 810: loss 20.823204040527344\n",
      "Itérations_train 811: loss 22.995866775512695\n",
      "Itérations_test 811: loss 20.822895050048828\n",
      "Itérations_train 812: loss 22.995805740356445\n",
      "Itérations_test 812: loss 20.82259178161621\n",
      "Itérations_train 813: loss 22.99573516845703\n",
      "Itérations_test 813: loss 20.822288513183594\n",
      "Itérations_train 814: loss 22.99565887451172\n",
      "Itérations_test 814: loss 20.821983337402344\n",
      "Itérations_train 815: loss 22.9955997467041\n",
      "Itérations_test 815: loss 20.82168197631836\n",
      "Itérations_train 816: loss 22.99553108215332\n",
      "Itérations_test 816: loss 20.821380615234375\n",
      "Itérations_train 817: loss 22.995471954345703\n",
      "Itérations_test 817: loss 20.821081161499023\n",
      "Itérations_train 818: loss 22.995420455932617\n",
      "Itérations_test 818: loss 20.820789337158203\n",
      "Itérations_train 819: loss 22.995349884033203\n",
      "Itérations_test 819: loss 20.820486068725586\n",
      "Itérations_train 820: loss 22.995285034179688\n",
      "Itérations_test 820: loss 20.820199966430664\n",
      "Itérations_train 821: loss 22.995214462280273\n",
      "Itérations_test 821: loss 20.819896697998047\n",
      "Itérations_train 822: loss 22.99514389038086\n",
      "Itérations_test 822: loss 20.819608688354492\n",
      "Itérations_train 823: loss 22.99509048461914\n",
      "Itérations_test 823: loss 20.819311141967773\n",
      "Itérations_train 824: loss 22.995027542114258\n",
      "Itérations_test 824: loss 20.81901741027832\n",
      "Itérations_train 825: loss 22.994966506958008\n",
      "Itérations_test 825: loss 20.81873321533203\n",
      "Itérations_train 826: loss 22.994892120361328\n",
      "Itérations_test 826: loss 20.818443298339844\n",
      "Itérations_train 827: loss 22.994836807250977\n",
      "Itérations_test 827: loss 20.81815528869629\n",
      "Itérations_train 828: loss 22.994779586791992\n",
      "Itérations_test 828: loss 20.8178653717041\n",
      "Itérations_train 829: loss 22.994726181030273\n",
      "Itérations_test 829: loss 20.817584991455078\n",
      "Itérations_train 830: loss 22.994657516479492\n",
      "Itérations_test 830: loss 20.81730842590332\n",
      "Itérations_train 831: loss 22.994598388671875\n",
      "Itérations_test 831: loss 20.81702423095703\n",
      "Itérations_train 832: loss 22.994543075561523\n",
      "Itérations_test 832: loss 20.81673812866211\n",
      "Itérations_train 833: loss 22.99448585510254\n",
      "Itérations_test 833: loss 20.81646156311035\n",
      "Itérations_train 834: loss 22.99441909790039\n",
      "Itérations_test 834: loss 20.816179275512695\n",
      "Itérations_train 835: loss 22.994359970092773\n",
      "Itérations_test 835: loss 20.81591033935547\n",
      "Itérations_train 836: loss 22.994308471679688\n",
      "Itérations_test 836: loss 20.815624237060547\n",
      "Itérations_train 837: loss 22.994251251220703\n",
      "Itérations_test 837: loss 20.81534767150879\n",
      "Itérations_train 838: loss 22.994184494018555\n",
      "Itérations_test 838: loss 20.81507682800293\n",
      "Itérations_train 839: loss 22.99413299560547\n",
      "Itérations_test 839: loss 20.81479835510254\n",
      "Itérations_train 840: loss 22.99407196044922\n",
      "Itérations_test 840: loss 20.814529418945312\n",
      "Itérations_train 841: loss 22.994029998779297\n",
      "Itérations_test 841: loss 20.814252853393555\n",
      "Itérations_train 842: loss 22.993967056274414\n",
      "Itérations_test 842: loss 20.813989639282227\n",
      "Itérations_train 843: loss 22.99390983581543\n",
      "Itérations_test 843: loss 20.813720703125\n",
      "Itérations_train 844: loss 22.993854522705078\n",
      "Itérations_test 844: loss 20.813447952270508\n",
      "Itérations_train 845: loss 22.993806838989258\n",
      "Itérations_test 845: loss 20.813180923461914\n",
      "Itérations_train 846: loss 22.993743896484375\n",
      "Itérations_test 846: loss 20.812917709350586\n",
      "Itérations_train 847: loss 22.993703842163086\n",
      "Itérations_test 847: loss 20.812644958496094\n",
      "Itérations_train 848: loss 22.993648529052734\n",
      "Itérations_test 848: loss 20.81239128112793\n",
      "Itérations_train 849: loss 22.993579864501953\n",
      "Itérations_test 849: loss 20.812118530273438\n",
      "Itérations_train 850: loss 22.99353790283203\n",
      "Itérations_test 850: loss 20.811866760253906\n",
      "Itérations_train 851: loss 22.99347686767578\n",
      "Itérations_test 851: loss 20.811588287353516\n",
      "Itérations_train 852: loss 22.993425369262695\n",
      "Itérations_test 852: loss 20.81134796142578\n",
      "Itérations_train 853: loss 22.99337387084961\n",
      "Itérations_test 853: loss 20.81108856201172\n",
      "Itérations_train 854: loss 22.99332618713379\n",
      "Itérations_test 854: loss 20.810827255249023\n",
      "Itérations_train 855: loss 22.993270874023438\n",
      "Itérations_test 855: loss 20.81056785583496\n",
      "Itérations_train 856: loss 22.993227005004883\n",
      "Itérations_test 856: loss 20.810321807861328\n",
      "Itérations_train 857: loss 22.993179321289062\n",
      "Itérations_test 857: loss 20.810062408447266\n",
      "Itérations_train 858: loss 22.993135452270508\n",
      "Itérations_test 858: loss 20.809803009033203\n",
      "Itérations_train 859: loss 22.993072509765625\n",
      "Itérations_test 859: loss 20.809547424316406\n",
      "Itérations_train 860: loss 22.993024826049805\n",
      "Itérations_test 860: loss 20.809297561645508\n",
      "Itérations_train 861: loss 22.99298095703125\n",
      "Itérations_test 861: loss 20.80904197692871\n",
      "Itérations_train 862: loss 22.992929458618164\n",
      "Itérations_test 862: loss 20.808805465698242\n",
      "Itérations_train 863: loss 22.992877960205078\n",
      "Itérations_test 863: loss 20.808549880981445\n",
      "Itérations_train 864: loss 22.992830276489258\n",
      "Itérations_test 864: loss 20.808305740356445\n",
      "Itérations_train 865: loss 22.99278450012207\n",
      "Itérations_test 865: loss 20.808061599731445\n",
      "Itérations_train 866: loss 22.99273681640625\n",
      "Itérations_test 866: loss 20.80780792236328\n",
      "Itérations_train 867: loss 22.992694854736328\n",
      "Itérations_test 867: loss 20.807567596435547\n",
      "Itérations_train 868: loss 22.992631912231445\n",
      "Itérations_test 868: loss 20.807321548461914\n",
      "Itérations_train 869: loss 22.99260711669922\n",
      "Itérations_test 869: loss 20.80708122253418\n",
      "Itérations_train 870: loss 22.992542266845703\n",
      "Itérations_test 870: loss 20.806833267211914\n",
      "Itérations_train 871: loss 22.992490768432617\n",
      "Itérations_test 871: loss 20.806591033935547\n",
      "Itérations_train 872: loss 22.99245262145996\n",
      "Itérations_test 872: loss 20.806350708007812\n",
      "Itérations_train 873: loss 22.992406845092773\n",
      "Itérations_test 873: loss 20.806114196777344\n",
      "Itérations_train 874: loss 22.992368698120117\n",
      "Itérations_test 874: loss 20.805877685546875\n",
      "Itérations_train 875: loss 22.992307662963867\n",
      "Itérations_test 875: loss 20.805639266967773\n",
      "Itérations_train 876: loss 22.992265701293945\n",
      "Itérations_test 876: loss 20.8054141998291\n",
      "Itérations_train 877: loss 22.992225646972656\n",
      "Itérations_test 877: loss 20.805166244506836\n",
      "Itérations_train 878: loss 22.99217414855957\n",
      "Itérations_test 878: loss 20.804933547973633\n",
      "Itérations_train 879: loss 22.992149353027344\n",
      "Itérations_test 879: loss 20.80470085144043\n",
      "Itérations_train 880: loss 22.992097854614258\n",
      "Itérations_test 880: loss 20.804462432861328\n",
      "Itérations_train 881: loss 22.992053985595703\n",
      "Itérations_test 881: loss 20.804237365722656\n",
      "Itérations_train 882: loss 22.992013931274414\n",
      "Itérations_test 882: loss 20.80400848388672\n",
      "Itérations_train 883: loss 22.99197769165039\n",
      "Itérations_test 883: loss 20.803773880004883\n",
      "Itérations_train 884: loss 22.991931915283203\n",
      "Itérations_test 884: loss 20.803550720214844\n",
      "Itérations_train 885: loss 22.991891860961914\n",
      "Itérations_test 885: loss 20.803316116333008\n",
      "Itérations_train 886: loss 22.99184799194336\n",
      "Itérations_test 886: loss 20.80309295654297\n",
      "Itérations_train 887: loss 22.991804122924805\n",
      "Itérations_test 887: loss 20.802871704101562\n",
      "Itérations_train 888: loss 22.991764068603516\n",
      "Itérations_test 888: loss 20.80263328552246\n",
      "Itérations_train 889: loss 22.99172592163086\n",
      "Itérations_test 889: loss 20.802412033081055\n",
      "Itérations_train 890: loss 22.991683959960938\n",
      "Itérations_test 890: loss 20.80219268798828\n",
      "Itérations_train 891: loss 22.991641998291016\n",
      "Itérations_test 891: loss 20.80196762084961\n",
      "Itérations_train 892: loss 22.991601943969727\n",
      "Itérations_test 892: loss 20.801748275756836\n",
      "Itérations_train 893: loss 22.991558074951172\n",
      "Itérations_test 893: loss 20.80152702331543\n",
      "Itérations_train 894: loss 22.99152183532715\n",
      "Itérations_test 894: loss 20.801307678222656\n",
      "Itérations_train 895: loss 22.991477966308594\n",
      "Itérations_test 895: loss 20.801082611083984\n",
      "Itérations_train 896: loss 22.991437911987305\n",
      "Itérations_test 896: loss 20.80086326599121\n",
      "Itérations_train 897: loss 22.99138641357422\n",
      "Itérations_test 897: loss 20.800643920898438\n",
      "Itérations_train 898: loss 22.99135971069336\n",
      "Itérations_test 898: loss 20.800426483154297\n",
      "Itérations_train 899: loss 22.991331100463867\n",
      "Itérations_test 899: loss 20.80021858215332\n",
      "Itérations_train 900: loss 22.991291046142578\n",
      "Itérations_test 900: loss 20.800006866455078\n",
      "Itérations_train 901: loss 22.991243362426758\n",
      "Itérations_test 901: loss 20.799779891967773\n",
      "Itérations_train 902: loss 22.99120330810547\n",
      "Itérations_test 902: loss 20.79956817626953\n",
      "Itérations_train 903: loss 22.99115562438965\n",
      "Itérations_test 903: loss 20.799352645874023\n",
      "Itérations_train 904: loss 22.99112892150879\n",
      "Itérations_test 904: loss 20.799137115478516\n",
      "Itérations_train 905: loss 22.99109649658203\n",
      "Itérations_test 905: loss 20.798934936523438\n",
      "Itérations_train 906: loss 22.991060256958008\n",
      "Itérations_test 906: loss 20.798725128173828\n",
      "Itérations_train 907: loss 22.99102020263672\n",
      "Itérations_test 907: loss 20.79850959777832\n",
      "Itérations_train 908: loss 22.990985870361328\n",
      "Itérations_test 908: loss 20.79830551147461\n",
      "Itérations_train 909: loss 22.990949630737305\n",
      "Itérations_test 909: loss 20.798095703125\n",
      "Itérations_train 910: loss 22.99090576171875\n",
      "Itérations_test 910: loss 20.797887802124023\n",
      "Itérations_train 911: loss 22.990886688232422\n",
      "Itérations_test 911: loss 20.79768180847168\n",
      "Itérations_train 912: loss 22.990842819213867\n",
      "Itérations_test 912: loss 20.797470092773438\n",
      "Itérations_train 913: loss 22.990814208984375\n",
      "Itérations_test 913: loss 20.797273635864258\n",
      "Itérations_train 914: loss 22.99076271057129\n",
      "Itérations_test 914: loss 20.797069549560547\n",
      "Itérations_train 915: loss 22.990739822387695\n",
      "Itérations_test 915: loss 20.796863555908203\n",
      "Itérations_train 916: loss 22.990703582763672\n",
      "Itérations_test 916: loss 20.796661376953125\n",
      "Itérations_train 917: loss 22.99067497253418\n",
      "Itérations_test 917: loss 20.79646110534668\n",
      "Itérations_train 918: loss 22.990631103515625\n",
      "Itérations_test 918: loss 20.796253204345703\n",
      "Itérations_train 919: loss 22.9906005859375\n",
      "Itérations_test 919: loss 20.79605484008789\n",
      "Itérations_train 920: loss 22.990568161010742\n",
      "Itérations_test 920: loss 20.795848846435547\n",
      "Itérations_train 921: loss 22.990537643432617\n",
      "Itérations_test 921: loss 20.7956600189209\n",
      "Itérations_train 922: loss 22.990495681762695\n",
      "Itérations_test 922: loss 20.795455932617188\n",
      "Itérations_train 923: loss 22.99046516418457\n",
      "Itérations_test 923: loss 20.79525375366211\n",
      "Itérations_train 924: loss 22.990432739257812\n",
      "Itérations_test 924: loss 20.795061111450195\n",
      "Itérations_train 925: loss 22.990394592285156\n",
      "Itérations_test 925: loss 20.794870376586914\n",
      "Itérations_train 926: loss 22.990365982055664\n",
      "Itérations_test 926: loss 20.794675827026367\n",
      "Itérations_train 927: loss 22.990333557128906\n",
      "Itérations_test 927: loss 20.79447364807129\n",
      "Itérations_train 928: loss 22.990306854248047\n",
      "Itérations_test 928: loss 20.794288635253906\n",
      "Itérations_train 929: loss 22.99027442932129\n",
      "Itérations_test 929: loss 20.794092178344727\n",
      "Itérations_train 930: loss 22.990238189697266\n",
      "Itérations_test 930: loss 20.793903350830078\n",
      "Itérations_train 931: loss 22.99020767211914\n",
      "Itérations_test 931: loss 20.793712615966797\n",
      "Itérations_train 932: loss 22.990171432495117\n",
      "Itérations_test 932: loss 20.793514251708984\n",
      "Itérations_train 933: loss 22.990131378173828\n",
      "Itérations_test 933: loss 20.793325424194336\n",
      "Itérations_train 934: loss 22.99011993408203\n",
      "Itérations_test 934: loss 20.79314422607422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itérations_train 935: loss 22.990079879760742\n",
      "Itérations_test 935: loss 20.792953491210938\n",
      "Itérations_train 936: loss 22.99005126953125\n",
      "Itérations_test 936: loss 20.792757034301758\n",
      "Itérations_train 937: loss 22.990018844604492\n",
      "Itérations_test 937: loss 20.792570114135742\n",
      "Itérations_train 938: loss 22.989992141723633\n",
      "Itérations_test 938: loss 20.792381286621094\n",
      "Itérations_train 939: loss 22.989959716796875\n",
      "Itérations_test 939: loss 20.79219627380371\n",
      "Itérations_train 940: loss 22.98993682861328\n",
      "Itérations_test 940: loss 20.79201889038086\n",
      "Itérations_train 941: loss 22.98990821838379\n",
      "Itérations_test 941: loss 20.79183006286621\n",
      "Itérations_train 942: loss 22.989871978759766\n",
      "Itérations_test 942: loss 20.791648864746094\n",
      "Itérations_train 943: loss 22.98984146118164\n",
      "Itérations_test 943: loss 20.791460037231445\n",
      "Itérations_train 944: loss 22.989818572998047\n",
      "Itérations_test 944: loss 20.79128074645996\n",
      "Itérations_train 945: loss 22.989789962768555\n",
      "Itérations_test 945: loss 20.79110336303711\n",
      "Itérations_train 946: loss 22.989757537841797\n",
      "Itérations_test 946: loss 20.790918350219727\n",
      "Itérations_train 947: loss 22.989721298217773\n",
      "Itérations_test 947: loss 20.790733337402344\n",
      "Itérations_train 948: loss 22.98969078063965\n",
      "Itérations_test 948: loss 20.79055404663086\n",
      "Itérations_train 949: loss 22.98967742919922\n",
      "Itérations_test 949: loss 20.790374755859375\n",
      "Itérations_train 950: loss 22.989635467529297\n",
      "Itérations_test 950: loss 20.790193557739258\n",
      "Itérations_train 951: loss 22.989614486694336\n",
      "Itérations_test 951: loss 20.790019989013672\n",
      "Itérations_train 952: loss 22.989595413208008\n",
      "Itérations_test 952: loss 20.789844512939453\n",
      "Itérations_train 953: loss 22.989566802978516\n",
      "Itérations_test 953: loss 20.789669036865234\n",
      "Itérations_train 954: loss 22.989547729492188\n",
      "Itérations_test 954: loss 20.78949546813965\n",
      "Itérations_train 955: loss 22.98951530456543\n",
      "Itérations_test 955: loss 20.789310455322266\n",
      "Itérations_train 956: loss 22.98948860168457\n",
      "Itérations_test 956: loss 20.78913688659668\n",
      "Itérations_train 957: loss 22.98945426940918\n",
      "Itérations_test 957: loss 20.78896141052246\n",
      "Itérations_train 958: loss 22.989437103271484\n",
      "Itérations_test 958: loss 20.78879165649414\n",
      "Itérations_train 959: loss 22.989404678344727\n",
      "Itérations_test 959: loss 20.788623809814453\n",
      "Itérations_train 960: loss 22.989383697509766\n",
      "Itérations_test 960: loss 20.7884464263916\n",
      "Itérations_train 961: loss 22.989349365234375\n",
      "Itérations_test 961: loss 20.78827476501465\n",
      "Itérations_train 962: loss 22.989320755004883\n",
      "Itérations_test 962: loss 20.788105010986328\n",
      "Itérations_train 963: loss 22.989295959472656\n",
      "Itérations_test 963: loss 20.787935256958008\n",
      "Itérations_train 964: loss 22.98926544189453\n",
      "Itérations_test 964: loss 20.787765502929688\n",
      "Itérations_train 965: loss 22.9892578125\n",
      "Itérations_test 965: loss 20.787593841552734\n",
      "Itérations_train 966: loss 22.989225387573242\n",
      "Itérations_test 966: loss 20.787425994873047\n",
      "Itérations_train 967: loss 22.98919677734375\n",
      "Itérations_test 967: loss 20.78725242614746\n",
      "Itérations_train 968: loss 22.989173889160156\n",
      "Itérations_test 968: loss 20.787090301513672\n",
      "Itérations_train 969: loss 22.989145278930664\n",
      "Itérations_test 969: loss 20.78692054748535\n",
      "Itérations_train 970: loss 22.98912239074707\n",
      "Itérations_test 970: loss 20.786758422851562\n",
      "Itérations_train 971: loss 22.98910140991211\n",
      "Itérations_test 971: loss 20.78659439086914\n",
      "Itérations_train 972: loss 22.98907470703125\n",
      "Itérations_test 972: loss 20.78642463684082\n",
      "Itérations_train 973: loss 22.98906135559082\n",
      "Itérations_test 973: loss 20.7862606048584\n",
      "Itérations_train 974: loss 22.989038467407227\n",
      "Itérations_test 974: loss 20.786096572875977\n",
      "Itérations_train 975: loss 22.989002227783203\n",
      "Itérations_test 975: loss 20.785932540893555\n",
      "Itérations_train 976: loss 22.98898696899414\n",
      "Itérations_test 976: loss 20.785770416259766\n",
      "Itérations_train 977: loss 22.98896598815918\n",
      "Itérations_test 977: loss 20.78560447692871\n",
      "Itérations_train 978: loss 22.988935470581055\n",
      "Itérations_test 978: loss 20.785449981689453\n",
      "Itérations_train 979: loss 22.988903045654297\n",
      "Itérations_test 979: loss 20.785282135009766\n",
      "Itérations_train 980: loss 22.98889923095703\n",
      "Itérations_test 980: loss 20.785127639770508\n",
      "Itérations_train 981: loss 22.988861083984375\n",
      "Itérations_test 981: loss 20.784963607788086\n",
      "Itérations_train 982: loss 22.988840103149414\n",
      "Itérations_test 982: loss 20.784805297851562\n",
      "Itérations_train 983: loss 22.98882484436035\n",
      "Itérations_test 983: loss 20.784645080566406\n",
      "Itérations_train 984: loss 22.98879623413086\n",
      "Itérations_test 984: loss 20.78449058532715\n",
      "Itérations_train 985: loss 22.988771438598633\n",
      "Itérations_test 985: loss 20.784330368041992\n",
      "Itérations_train 986: loss 22.98874855041504\n",
      "Itérations_test 986: loss 20.784177780151367\n",
      "Itérations_train 987: loss 22.988725662231445\n",
      "Itérations_test 987: loss 20.784013748168945\n",
      "Itérations_train 988: loss 22.988704681396484\n",
      "Itérations_test 988: loss 20.78386116027832\n",
      "Itérations_train 989: loss 22.98868179321289\n",
      "Itérations_test 989: loss 20.78371238708496\n",
      "Itérations_train 990: loss 22.988666534423828\n",
      "Itérations_test 990: loss 20.783559799194336\n",
      "Itérations_train 991: loss 22.988636016845703\n",
      "Itérations_test 991: loss 20.783403396606445\n",
      "Itérations_train 992: loss 22.98862075805664\n",
      "Itérations_test 992: loss 20.783246994018555\n",
      "Itérations_train 993: loss 22.988601684570312\n",
      "Itérations_test 993: loss 20.783098220825195\n",
      "Itérations_train 994: loss 22.98856544494629\n",
      "Itérations_test 994: loss 20.782936096191406\n",
      "Itérations_train 995: loss 22.988557815551758\n",
      "Itérations_test 995: loss 20.78278350830078\n",
      "Itérations_train 996: loss 22.988536834716797\n",
      "Itérations_test 996: loss 20.78264045715332\n",
      "Itérations_train 997: loss 22.988510131835938\n",
      "Itérations_test 997: loss 20.782493591308594\n",
      "Itérations_train 998: loss 22.988494873046875\n",
      "Itérations_test 998: loss 20.782350540161133\n",
      "Itérations_train 999: loss 22.988473892211914\n",
      "Itérations_test 999: loss 20.782188415527344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# complete pass/ same result as earlier\n",
    "stochastic_gradient_descent(x_train_input, x_test_input, y_train_input, y_test_input, Model = Linear,\\\n",
    "                                Loss = MSE, nb_epochs = 1000, batch_size = len(x_train_input), learning_rate = 0.01,\\\n",
    "                                writer = writer, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# SGD\\nstochastic_gradient_descent(x_train_input, x_test_input, y_train_input, y_test_input, Model = Linear,                                Loss = MSE, nb_epochs = 1000, batch_size = 1, learning_rate = 0.01,                                writer = writer, verbose = True)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# SGD\n",
    "stochastic_gradient_descent(x_train_input, x_test_input, y_train_input, y_test_input, Model = Linear,\\\n",
    "                                Loss = MSE, nb_epochs = 1000, batch_size = 1, learning_rate = 0.01,\\\n",
    "                                writer = writer, verbose = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Mini_batch\\nstochastic_gradient_descent(x_train_input, x_test_input, y_train_input, y_test_input, Model = Linear,                                Loss = MSE, nb_epochs = 1000, batch_size = 50, learning_rate = 0.001,                                writer = writer, verbose = True)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Mini_batch\n",
    "stochastic_gradient_descent(x_train_input, x_test_input, y_train_input, y_test_input, Model = Linear,\\\n",
    "                                Loss = MSE, nb_epochs = 1000, batch_size = 50, learning_rate = 0.001,\\\n",
    "                                writer = writer, verbose = True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Open TensorBoard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
